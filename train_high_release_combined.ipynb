{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum class size: 400\n",
      "Class sizes: {'Shot': 400, 'Background': 867}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_minimum_class_size(data_directory):\n",
    "    class_sizes = {}\n",
    "    for class_dir in os.listdir(data_directory):\n",
    "        class_path = os.path.join(data_directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_sizes[class_dir] = len([name for name in os.listdir(class_path) if name.endswith('.npy')])\n",
    "    min_size = min(class_sizes.values())\n",
    "    return min_size, class_sizes\n",
    "\n",
    "data_directory = 'splitted_datapoints_shot/val'\n",
    "min_size, class_sizes = find_minimum_class_size(data_directory)\n",
    "print(\"Minimum class size:\", min_size)\n",
    "print(\"Class sizes:\", class_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data balanced and copied to: balanced_splitted_datapoints_shot/val\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "def balance_classes(data_directory, output_directory, min_size):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    for class_dir in os.listdir(data_directory):\n",
    "        class_path = os.path.join(data_directory, class_dir)\n",
    "        if os.path.isdir(class_path):\n",
    "            files = [name for name in os.listdir(class_path) if name.endswith('.npy')]\n",
    "            np.random.shuffle(files)  # Shuffle to avoid bias\n",
    "            selected_files = files[:min_size]  # Select a subset based on the minimum size\n",
    "            \n",
    "            # Create a directory for the current class in the output directory\n",
    "            output_class_path = os.path.join(output_directory, class_dir)\n",
    "            if not os.path.exists(output_class_path):\n",
    "                os.makedirs(output_class_path)\n",
    "                \n",
    "            # Copy selected files to the new directory\n",
    "            for file_name in selected_files:\n",
    "                src_file_path = os.path.join(class_path, file_name)\n",
    "                dst_file_path = os.path.join(output_class_path, file_name)\n",
    "                shutil.copy(src_file_path, dst_file_path)\n",
    "    print(\"Data balanced and copied to:\", output_directory)\n",
    "\n",
    "# Specify the output directory where balanced data will be stored\n",
    "output_directory = 'balanced_splitted_datapoints_shot/val'\n",
    "balance_classes(data_directory, output_directory, min_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class NPYDataset(Dataset):\n",
    "    def __init__(self, main_directory):\n",
    "        super().__init__()\n",
    "        self.main_directory = main_directory\n",
    "        self.filenames = self._load_filenames()\n",
    "\n",
    "    def _load_filenames(self):\n",
    "        # Traverse subdirectories to find all .npy files\n",
    "        filenames = []\n",
    "        for root, _, files in os.walk(self.main_directory):\n",
    "            for file in files:\n",
    "                if file.endswith('.npy'):\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    filenames.append(full_path)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.filenames[idx]\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        keypoints = np.ascontiguousarray(data['data'], dtype=np.float32)\n",
    "        label = np.float32(data['label'])\n",
    "        # Convert to tensors\n",
    "        keypoints = torch.tensor(keypoints)\n",
    "        keypoints = keypoints[:, :, :-1, :]\n",
    "        label = torch.tensor(label)\n",
    "        return keypoints, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# DataLoader setup\n",
    "data_directory = '7-segment_datasets/highRel_splitted_dataset_backall_included/train'\n",
    "dataset = NPYDataset(data_directory)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "val_dataset = NPYDataset('7-segment_datasets/highRel_splitted_dataset_backall_included/val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[1.]])\n",
      "torch.Size([1, 1, 7, 33, 2]) tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "for keypoints, labels in dataloader:\n",
    "    # keypoints = keypoints.squeeze(0)\n",
    "    print(keypoints.shape, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, num_classes):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.segment_length, self.keypoints, self.coords = input_shape\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=self.keypoints * self.coords, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(input_size=32, hidden_size=hidden_dim, batch_first=True)\n",
    "        # Change to output a single value for binary classification\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Only one output neuron for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), self.keypoints * self.coords, self.segment_length)\n",
    "        x = self.conv1d(x)\n",
    "        x = torch.relu(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)  # Sigmoid activation for binary classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume 'model' is an instance of your model class\n",
    "model = ConvLSTMModel(input_shape=(7, 33, 3), hidden_dim=64, num_classes=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for evaluating the model on the validation set\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for keypoints, labels in val_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(keypoints)  # outputs shape should be [batch_size, 1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_labels = outputs.round()  # Round the outputs to get the final predictions for binary classification\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.6710, Train Accuracy: 58.15%\n",
      "Epoch 1/500 - Validation Loss: 0.6854, Validation Accuracy: 0.5775\n",
      "Best model saved at epoch 1 with val acc: 0.5775\n",
      "Epoch [2/500], Train Loss: 0.6806, Train Accuracy: 55.96%\n",
      "Epoch 2/500 - Validation Loss: 0.6908, Validation Accuracy: 0.5732\n",
      "Epoch [3/500], Train Loss: 0.6783, Train Accuracy: 57.35%\n",
      "Epoch 3/500 - Validation Loss: 0.6638, Validation Accuracy: 0.5732\n",
      "Epoch [4/500], Train Loss: 0.6710, Train Accuracy: 57.72%\n",
      "Epoch 4/500 - Validation Loss: 0.6735, Validation Accuracy: 0.5690\n",
      "Epoch [5/500], Train Loss: 0.6728, Train Accuracy: 58.57%\n",
      "Epoch 5/500 - Validation Loss: 0.6645, Validation Accuracy: 0.5732\n",
      "Epoch [6/500], Train Loss: 0.6747, Train Accuracy: 56.98%\n",
      "Epoch 6/500 - Validation Loss: 0.6645, Validation Accuracy: 0.5754\n",
      "Epoch [7/500], Train Loss: 0.6669, Train Accuracy: 57.35%\n",
      "Epoch 7/500 - Validation Loss: 0.6803, Validation Accuracy: 0.5732\n",
      "Epoch [8/500], Train Loss: 0.6713, Train Accuracy: 59.05%\n",
      "Epoch 8/500 - Validation Loss: 0.6713, Validation Accuracy: 0.5732\n",
      "Epoch [9/500], Train Loss: 0.6713, Train Accuracy: 57.19%\n",
      "Epoch 9/500 - Validation Loss: 0.6626, Validation Accuracy: 0.5732\n",
      "Epoch [10/500], Train Loss: 0.6753, Train Accuracy: 57.56%\n",
      "Epoch 10/500 - Validation Loss: 0.6793, Validation Accuracy: 0.5732\n",
      "Epoch [11/500], Train Loss: 0.6764, Train Accuracy: 57.35%\n",
      "Epoch 11/500 - Validation Loss: 0.6760, Validation Accuracy: 0.5732\n",
      "Epoch [12/500], Train Loss: 0.6754, Train Accuracy: 56.98%\n",
      "Epoch 12/500 - Validation Loss: 0.6710, Validation Accuracy: 0.5732\n",
      "Epoch [13/500], Train Loss: 0.6730, Train Accuracy: 57.35%\n",
      "Epoch 13/500 - Validation Loss: 0.6754, Validation Accuracy: 0.5732\n",
      "Epoch [14/500], Train Loss: 0.6730, Train Accuracy: 58.36%\n",
      "Epoch 14/500 - Validation Loss: 0.6725, Validation Accuracy: 0.6242\n",
      "Best model saved at epoch 14 with val acc: 0.6242\n",
      "Epoch [15/500], Train Loss: 0.6675, Train Accuracy: 60.28%\n",
      "Epoch 15/500 - Validation Loss: 0.6603, Validation Accuracy: 0.5732\n",
      "Epoch [16/500], Train Loss: 0.6596, Train Accuracy: 57.56%\n",
      "Epoch 16/500 - Validation Loss: 0.6482, Validation Accuracy: 0.6221\n",
      "Epoch [17/500], Train Loss: 0.6564, Train Accuracy: 59.21%\n",
      "Epoch 17/500 - Validation Loss: 0.6504, Validation Accuracy: 0.6369\n",
      "Best model saved at epoch 17 with val acc: 0.6369\n",
      "Epoch [18/500], Train Loss: 0.6555, Train Accuracy: 59.74%\n",
      "Epoch 18/500 - Validation Loss: 0.6574, Validation Accuracy: 0.5860\n",
      "Epoch [19/500], Train Loss: 0.6629, Train Accuracy: 57.88%\n",
      "Epoch 19/500 - Validation Loss: 0.6480, Validation Accuracy: 0.5817\n",
      "Epoch [20/500], Train Loss: 0.6461, Train Accuracy: 60.12%\n",
      "Epoch 20/500 - Validation Loss: 0.6344, Validation Accuracy: 0.6454\n",
      "Best model saved at epoch 20 with val acc: 0.6454\n",
      "Epoch [21/500], Train Loss: 0.6454, Train Accuracy: 59.00%\n",
      "Epoch 21/500 - Validation Loss: 0.6309, Validation Accuracy: 0.6433\n",
      "Epoch [22/500], Train Loss: 0.6345, Train Accuracy: 63.37%\n",
      "Epoch 22/500 - Validation Loss: 0.6283, Validation Accuracy: 0.6433\n",
      "Epoch [23/500], Train Loss: 0.6195, Train Accuracy: 62.19%\n",
      "Epoch 23/500 - Validation Loss: 0.6057, Validation Accuracy: 0.5924\n",
      "Epoch [24/500], Train Loss: 0.5982, Train Accuracy: 67.41%\n",
      "Epoch 24/500 - Validation Loss: 0.5897, Validation Accuracy: 0.7070\n",
      "Best model saved at epoch 24 with val acc: 0.7070\n",
      "Epoch [25/500], Train Loss: 0.5941, Train Accuracy: 66.35%\n",
      "Epoch 25/500 - Validation Loss: 0.5874, Validation Accuracy: 0.6624\n",
      "Epoch [26/500], Train Loss: 0.5707, Train Accuracy: 71.62%\n",
      "Epoch 26/500 - Validation Loss: 0.5777, Validation Accuracy: 0.7049\n",
      "Epoch [27/500], Train Loss: 0.5762, Train Accuracy: 67.89%\n",
      "Epoch 27/500 - Validation Loss: 0.5750, Validation Accuracy: 0.6688\n",
      "Epoch [28/500], Train Loss: 0.5569, Train Accuracy: 70.87%\n",
      "Epoch 28/500 - Validation Loss: 0.5497, Validation Accuracy: 0.7219\n",
      "Best model saved at epoch 28 with val acc: 0.7219\n",
      "Epoch [29/500], Train Loss: 0.5449, Train Accuracy: 72.47%\n",
      "Epoch 29/500 - Validation Loss: 0.5335, Validation Accuracy: 0.7325\n",
      "Best model saved at epoch 29 with val acc: 0.7325\n",
      "Epoch [30/500], Train Loss: 0.5284, Train Accuracy: 73.32%\n",
      "Epoch 30/500 - Validation Loss: 0.5215, Validation Accuracy: 0.7261\n",
      "Epoch [31/500], Train Loss: 0.5228, Train Accuracy: 73.80%\n",
      "Epoch 31/500 - Validation Loss: 0.5338, Validation Accuracy: 0.7304\n",
      "Epoch [32/500], Train Loss: 0.5240, Train Accuracy: 72.95%\n",
      "Epoch 32/500 - Validation Loss: 0.5200, Validation Accuracy: 0.6943\n",
      "Epoch [33/500], Train Loss: 0.5328, Train Accuracy: 72.31%\n",
      "Epoch 33/500 - Validation Loss: 0.5159, Validation Accuracy: 0.7346\n",
      "Best model saved at epoch 33 with val acc: 0.7346\n",
      "Epoch [34/500], Train Loss: 0.5193, Train Accuracy: 73.86%\n",
      "Epoch 34/500 - Validation Loss: 0.5128, Validation Accuracy: 0.7389\n",
      "Best model saved at epoch 34 with val acc: 0.7389\n",
      "Epoch [35/500], Train Loss: 0.5117, Train Accuracy: 73.38%\n",
      "Epoch 35/500 - Validation Loss: 0.5077, Validation Accuracy: 0.7431\n",
      "Best model saved at epoch 35 with val acc: 0.7431\n",
      "Epoch [36/500], Train Loss: 0.5153, Train Accuracy: 72.63%\n",
      "Epoch 36/500 - Validation Loss: 0.6060, Validation Accuracy: 0.6200\n",
      "Epoch [37/500], Train Loss: 0.5081, Train Accuracy: 73.54%\n",
      "Epoch 37/500 - Validation Loss: 0.5156, Validation Accuracy: 0.7261\n",
      "Epoch [38/500], Train Loss: 0.5057, Train Accuracy: 74.23%\n",
      "Epoch 38/500 - Validation Loss: 0.5581, Validation Accuracy: 0.6943\n",
      "Epoch [39/500], Train Loss: 0.5138, Train Accuracy: 73.70%\n",
      "Epoch 39/500 - Validation Loss: 0.5633, Validation Accuracy: 0.7091\n",
      "Epoch [40/500], Train Loss: 0.5057, Train Accuracy: 74.17%\n",
      "Epoch 40/500 - Validation Loss: 0.5134, Validation Accuracy: 0.7346\n",
      "Epoch [41/500], Train Loss: 0.5007, Train Accuracy: 73.86%\n",
      "Epoch 41/500 - Validation Loss: 0.4940, Validation Accuracy: 0.7367\n",
      "Epoch [42/500], Train Loss: 0.4917, Train Accuracy: 74.28%\n",
      "Epoch 42/500 - Validation Loss: 0.5057, Validation Accuracy: 0.7176\n",
      "Epoch [43/500], Train Loss: 0.4870, Train Accuracy: 74.44%\n",
      "Epoch 43/500 - Validation Loss: 0.5074, Validation Accuracy: 0.7261\n",
      "Epoch [44/500], Train Loss: 0.4860, Train Accuracy: 75.77%\n",
      "Epoch 44/500 - Validation Loss: 0.5105, Validation Accuracy: 0.7304\n",
      "Epoch [45/500], Train Loss: 0.4804, Train Accuracy: 73.48%\n",
      "Epoch 45/500 - Validation Loss: 0.5113, Validation Accuracy: 0.7197\n",
      "Epoch [46/500], Train Loss: 0.4859, Train Accuracy: 74.44%\n",
      "Epoch 46/500 - Validation Loss: 0.5208, Validation Accuracy: 0.7261\n",
      "Epoch [47/500], Train Loss: 0.4800, Train Accuracy: 75.13%\n",
      "Epoch 47/500 - Validation Loss: 0.4953, Validation Accuracy: 0.7240\n",
      "Epoch [48/500], Train Loss: 0.4769, Train Accuracy: 75.03%\n",
      "Epoch 48/500 - Validation Loss: 0.5220, Validation Accuracy: 0.7473\n",
      "Best model saved at epoch 48 with val acc: 0.7473\n",
      "Epoch [49/500], Train Loss: 0.4771, Train Accuracy: 75.08%\n",
      "Epoch 49/500 - Validation Loss: 0.4759, Validation Accuracy: 0.7452\n",
      "Epoch [50/500], Train Loss: 0.4845, Train Accuracy: 74.01%\n",
      "Epoch 50/500 - Validation Loss: 0.4826, Validation Accuracy: 0.7197\n",
      "Epoch [51/500], Train Loss: 0.4810, Train Accuracy: 74.28%\n",
      "Epoch 51/500 - Validation Loss: 0.4859, Validation Accuracy: 0.7282\n",
      "Epoch [52/500], Train Loss: 0.4813, Train Accuracy: 74.28%\n",
      "Epoch 52/500 - Validation Loss: 0.4966, Validation Accuracy: 0.6815\n",
      "Epoch [53/500], Train Loss: 0.4729, Train Accuracy: 75.03%\n",
      "Epoch 53/500 - Validation Loss: 0.5225, Validation Accuracy: 0.7134\n",
      "Epoch [54/500], Train Loss: 0.4794, Train Accuracy: 74.01%\n",
      "Epoch 54/500 - Validation Loss: 0.4950, Validation Accuracy: 0.7240\n",
      "Epoch [55/500], Train Loss: 0.4744, Train Accuracy: 74.60%\n",
      "Epoch 55/500 - Validation Loss: 0.5074, Validation Accuracy: 0.7346\n",
      "Epoch [56/500], Train Loss: 0.4819, Train Accuracy: 74.39%\n",
      "Epoch 56/500 - Validation Loss: 0.4972, Validation Accuracy: 0.6985\n",
      "Epoch [57/500], Train Loss: 0.4663, Train Accuracy: 75.19%\n",
      "Epoch 57/500 - Validation Loss: 0.5071, Validation Accuracy: 0.7282\n",
      "Epoch [58/500], Train Loss: 0.4784, Train Accuracy: 74.39%\n",
      "Epoch 58/500 - Validation Loss: 0.5129, Validation Accuracy: 0.7346\n",
      "Epoch [59/500], Train Loss: 0.4794, Train Accuracy: 74.55%\n",
      "Epoch 59/500 - Validation Loss: 0.4870, Validation Accuracy: 0.7304\n",
      "Epoch [60/500], Train Loss: 0.4767, Train Accuracy: 76.41%\n",
      "Epoch 60/500 - Validation Loss: 0.5023, Validation Accuracy: 0.7473\n",
      "Epoch [61/500], Train Loss: 0.4693, Train Accuracy: 75.40%\n",
      "Epoch 61/500 - Validation Loss: 0.4742, Validation Accuracy: 0.7325\n",
      "Epoch [62/500], Train Loss: 0.4655, Train Accuracy: 74.01%\n",
      "Epoch 62/500 - Validation Loss: 0.4772, Validation Accuracy: 0.7134\n",
      "Epoch [63/500], Train Loss: 0.4685, Train Accuracy: 75.72%\n",
      "Epoch 63/500 - Validation Loss: 0.5398, Validation Accuracy: 0.6773\n",
      "Epoch [64/500], Train Loss: 0.4711, Train Accuracy: 74.17%\n",
      "Epoch 64/500 - Validation Loss: 0.4839, Validation Accuracy: 0.7304\n",
      "Epoch [65/500], Train Loss: 0.4636, Train Accuracy: 75.72%\n",
      "Epoch 65/500 - Validation Loss: 0.4875, Validation Accuracy: 0.7410\n",
      "Epoch [66/500], Train Loss: 0.4807, Train Accuracy: 75.35%\n",
      "Epoch 66/500 - Validation Loss: 0.5037, Validation Accuracy: 0.7473\n",
      "Epoch [67/500], Train Loss: 0.4834, Train Accuracy: 74.23%\n",
      "Epoch 67/500 - Validation Loss: 0.4764, Validation Accuracy: 0.7389\n",
      "Epoch [68/500], Train Loss: 0.4736, Train Accuracy: 75.51%\n",
      "Epoch 68/500 - Validation Loss: 0.5876, Validation Accuracy: 0.6943\n",
      "Epoch [69/500], Train Loss: 0.4888, Train Accuracy: 73.96%\n",
      "Epoch 69/500 - Validation Loss: 0.4932, Validation Accuracy: 0.7261\n",
      "Epoch [70/500], Train Loss: 0.4687, Train Accuracy: 74.81%\n",
      "Epoch 70/500 - Validation Loss: 0.4770, Validation Accuracy: 0.7495\n",
      "Best model saved at epoch 70 with val acc: 0.7495\n",
      "Epoch [71/500], Train Loss: 0.4658, Train Accuracy: 75.56%\n",
      "Epoch 71/500 - Validation Loss: 0.4774, Validation Accuracy: 0.7240\n",
      "Epoch [72/500], Train Loss: 0.4811, Train Accuracy: 74.49%\n",
      "Epoch 72/500 - Validation Loss: 0.4903, Validation Accuracy: 0.7452\n",
      "Epoch [73/500], Train Loss: 0.4716, Train Accuracy: 75.24%\n",
      "Epoch 73/500 - Validation Loss: 0.4836, Validation Accuracy: 0.7155\n",
      "Epoch [74/500], Train Loss: 0.4615, Train Accuracy: 75.88%\n",
      "Epoch 74/500 - Validation Loss: 0.4839, Validation Accuracy: 0.7537\n",
      "Best model saved at epoch 74 with val acc: 0.7537\n",
      "Epoch [75/500], Train Loss: 0.4667, Train Accuracy: 75.29%\n",
      "Epoch 75/500 - Validation Loss: 0.5167, Validation Accuracy: 0.7367\n",
      "Epoch [76/500], Train Loss: 0.4640, Train Accuracy: 75.99%\n",
      "Epoch 76/500 - Validation Loss: 0.4738, Validation Accuracy: 0.7304\n",
      "Epoch [77/500], Train Loss: 0.4777, Train Accuracy: 75.08%\n",
      "Epoch 77/500 - Validation Loss: 0.5082, Validation Accuracy: 0.7431\n",
      "Epoch [78/500], Train Loss: 0.4686, Train Accuracy: 75.83%\n",
      "Epoch 78/500 - Validation Loss: 0.5290, Validation Accuracy: 0.6943\n",
      "Epoch [79/500], Train Loss: 0.4691, Train Accuracy: 74.65%\n",
      "Epoch 79/500 - Validation Loss: 0.4784, Validation Accuracy: 0.7176\n",
      "Epoch [80/500], Train Loss: 0.4787, Train Accuracy: 74.07%\n",
      "Epoch 80/500 - Validation Loss: 0.4797, Validation Accuracy: 0.7473\n",
      "Epoch [81/500], Train Loss: 0.4700, Train Accuracy: 75.24%\n",
      "Epoch 81/500 - Validation Loss: 0.4844, Validation Accuracy: 0.7091\n",
      "Epoch [82/500], Train Loss: 0.4640, Train Accuracy: 76.25%\n",
      "Epoch 82/500 - Validation Loss: 0.5028, Validation Accuracy: 0.7325\n",
      "Epoch [83/500], Train Loss: 0.4645, Train Accuracy: 75.29%\n",
      "Epoch 83/500 - Validation Loss: 0.5040, Validation Accuracy: 0.7367\n",
      "Epoch [84/500], Train Loss: 0.4652, Train Accuracy: 75.24%\n",
      "Epoch 84/500 - Validation Loss: 0.4916, Validation Accuracy: 0.7240\n",
      "Epoch [85/500], Train Loss: 0.4633, Train Accuracy: 74.76%\n",
      "Epoch 85/500 - Validation Loss: 0.4703, Validation Accuracy: 0.7410\n",
      "Epoch [86/500], Train Loss: 0.4725, Train Accuracy: 74.33%\n",
      "Epoch 86/500 - Validation Loss: 0.4868, Validation Accuracy: 0.7304\n",
      "Epoch [87/500], Train Loss: 0.4711, Train Accuracy: 75.13%\n",
      "Epoch 87/500 - Validation Loss: 0.5114, Validation Accuracy: 0.7516\n",
      "Epoch [88/500], Train Loss: 0.4756, Train Accuracy: 75.83%\n",
      "Epoch 88/500 - Validation Loss: 0.5136, Validation Accuracy: 0.7304\n",
      "Epoch [89/500], Train Loss: 0.4722, Train Accuracy: 75.08%\n",
      "Epoch 89/500 - Validation Loss: 0.4775, Validation Accuracy: 0.7155\n",
      "Epoch [90/500], Train Loss: 0.4707, Train Accuracy: 73.70%\n",
      "Epoch 90/500 - Validation Loss: 0.4832, Validation Accuracy: 0.7452\n",
      "Epoch [91/500], Train Loss: 0.4666, Train Accuracy: 75.56%\n",
      "Epoch 91/500 - Validation Loss: 0.4793, Validation Accuracy: 0.7219\n",
      "Epoch [92/500], Train Loss: 0.4674, Train Accuracy: 75.24%\n",
      "Epoch 92/500 - Validation Loss: 0.5464, Validation Accuracy: 0.7410\n",
      "Epoch [93/500], Train Loss: 0.4606, Train Accuracy: 74.39%\n",
      "Epoch 93/500 - Validation Loss: 0.4748, Validation Accuracy: 0.7176\n",
      "Epoch [94/500], Train Loss: 0.4692, Train Accuracy: 75.56%\n",
      "Epoch 94/500 - Validation Loss: 0.4692, Validation Accuracy: 0.7325\n",
      "Epoch [95/500], Train Loss: 0.4644, Train Accuracy: 76.73%\n",
      "Epoch 95/500 - Validation Loss: 0.5208, Validation Accuracy: 0.7410\n",
      "Epoch [96/500], Train Loss: 0.4544, Train Accuracy: 75.99%\n",
      "Epoch 96/500 - Validation Loss: 0.4824, Validation Accuracy: 0.7282\n",
      "Epoch [97/500], Train Loss: 0.4656, Train Accuracy: 75.13%\n",
      "Epoch 97/500 - Validation Loss: 0.4782, Validation Accuracy: 0.7346\n",
      "Epoch [98/500], Train Loss: 0.4565, Train Accuracy: 73.70%\n",
      "Epoch 98/500 - Validation Loss: 0.4983, Validation Accuracy: 0.7006\n",
      "Epoch [99/500], Train Loss: 0.4757, Train Accuracy: 73.96%\n",
      "Epoch 99/500 - Validation Loss: 0.4724, Validation Accuracy: 0.7197\n",
      "Epoch [100/500], Train Loss: 0.4629, Train Accuracy: 75.19%\n",
      "Epoch 100/500 - Validation Loss: 0.5125, Validation Accuracy: 0.7495\n",
      "Epoch [101/500], Train Loss: 0.4665, Train Accuracy: 73.91%\n",
      "Epoch 101/500 - Validation Loss: 0.4844, Validation Accuracy: 0.7113\n",
      "Epoch [102/500], Train Loss: 0.4710, Train Accuracy: 74.97%\n",
      "Epoch 102/500 - Validation Loss: 0.4928, Validation Accuracy: 0.7197\n",
      "Epoch [103/500], Train Loss: 0.4575, Train Accuracy: 75.61%\n",
      "Epoch 103/500 - Validation Loss: 0.4884, Validation Accuracy: 0.7473\n",
      "Epoch [104/500], Train Loss: 0.4620, Train Accuracy: 75.88%\n",
      "Epoch 104/500 - Validation Loss: 0.4911, Validation Accuracy: 0.7240\n",
      "Epoch [105/500], Train Loss: 0.4640, Train Accuracy: 75.45%\n",
      "Epoch 105/500 - Validation Loss: 0.4828, Validation Accuracy: 0.7176\n",
      "Epoch [106/500], Train Loss: 0.4581, Train Accuracy: 76.57%\n",
      "Epoch 106/500 - Validation Loss: 0.7342, Validation Accuracy: 0.6794\n",
      "Epoch [107/500], Train Loss: 0.4679, Train Accuracy: 76.14%\n",
      "Epoch 107/500 - Validation Loss: 0.5454, Validation Accuracy: 0.6688\n",
      "Epoch [108/500], Train Loss: 0.4731, Train Accuracy: 74.71%\n",
      "Epoch 108/500 - Validation Loss: 0.4817, Validation Accuracy: 0.7304\n",
      "Epoch [109/500], Train Loss: 0.4628, Train Accuracy: 75.61%\n",
      "Epoch 109/500 - Validation Loss: 0.5894, Validation Accuracy: 0.7028\n",
      "Epoch [110/500], Train Loss: 0.4625, Train Accuracy: 75.99%\n",
      "Epoch 110/500 - Validation Loss: 0.4993, Validation Accuracy: 0.7155\n",
      "Epoch [111/500], Train Loss: 0.4587, Train Accuracy: 76.62%\n",
      "Epoch 111/500 - Validation Loss: 0.4993, Validation Accuracy: 0.7410\n",
      "Epoch [112/500], Train Loss: 0.4691, Train Accuracy: 75.67%\n",
      "Epoch 112/500 - Validation Loss: 0.4876, Validation Accuracy: 0.7346\n",
      "Epoch [113/500], Train Loss: 0.4595, Train Accuracy: 75.88%\n",
      "Epoch 113/500 - Validation Loss: 0.4726, Validation Accuracy: 0.7495\n",
      "Epoch [114/500], Train Loss: 0.4755, Train Accuracy: 74.71%\n",
      "Epoch 114/500 - Validation Loss: 0.4689, Validation Accuracy: 0.7431\n",
      "Epoch [115/500], Train Loss: 0.4773, Train Accuracy: 75.45%\n",
      "Epoch 115/500 - Validation Loss: 0.4736, Validation Accuracy: 0.7495\n",
      "Epoch [116/500], Train Loss: 0.4649, Train Accuracy: 75.29%\n",
      "Epoch 116/500 - Validation Loss: 0.4928, Validation Accuracy: 0.7431\n",
      "Epoch [117/500], Train Loss: 0.4672, Train Accuracy: 76.62%\n",
      "Epoch 117/500 - Validation Loss: 0.4994, Validation Accuracy: 0.7261\n",
      "Epoch [118/500], Train Loss: 0.4729, Train Accuracy: 74.97%\n",
      "Epoch 118/500 - Validation Loss: 0.5083, Validation Accuracy: 0.7282\n",
      "Epoch [119/500], Train Loss: 0.4637, Train Accuracy: 76.09%\n",
      "Epoch 119/500 - Validation Loss: 0.4865, Validation Accuracy: 0.7452\n",
      "Epoch [120/500], Train Loss: 0.4678, Train Accuracy: 75.29%\n",
      "Epoch 120/500 - Validation Loss: 0.5006, Validation Accuracy: 0.7304\n",
      "Epoch [121/500], Train Loss: 0.4664, Train Accuracy: 75.67%\n",
      "Epoch 121/500 - Validation Loss: 0.4812, Validation Accuracy: 0.7410\n",
      "Epoch [122/500], Train Loss: 0.4631, Train Accuracy: 75.08%\n",
      "Epoch 122/500 - Validation Loss: 0.4706, Validation Accuracy: 0.7473\n",
      "Epoch [123/500], Train Loss: 0.4602, Train Accuracy: 75.88%\n",
      "Epoch 123/500 - Validation Loss: 0.5037, Validation Accuracy: 0.7049\n",
      "Epoch [124/500], Train Loss: 0.4726, Train Accuracy: 75.03%\n",
      "Epoch 124/500 - Validation Loss: 0.4881, Validation Accuracy: 0.7219\n",
      "Epoch [125/500], Train Loss: 0.4604, Train Accuracy: 75.61%\n",
      "Epoch 125/500 - Validation Loss: 0.5349, Validation Accuracy: 0.7219\n",
      "Epoch [126/500], Train Loss: 0.4604, Train Accuracy: 75.51%\n",
      "Epoch 126/500 - Validation Loss: 0.4832, Validation Accuracy: 0.7134\n",
      "Epoch [127/500], Train Loss: 0.4707, Train Accuracy: 74.65%\n",
      "Epoch 127/500 - Validation Loss: 0.4858, Validation Accuracy: 0.7346\n",
      "Epoch [128/500], Train Loss: 0.4791, Train Accuracy: 74.97%\n",
      "Epoch 128/500 - Validation Loss: 0.4737, Validation Accuracy: 0.7346\n",
      "Epoch [129/500], Train Loss: 0.4670, Train Accuracy: 74.65%\n",
      "Epoch 129/500 - Validation Loss: 0.4801, Validation Accuracy: 0.7431\n",
      "Epoch [130/500], Train Loss: 0.4565, Train Accuracy: 74.71%\n",
      "Epoch 130/500 - Validation Loss: 0.6653, Validation Accuracy: 0.6943\n",
      "Epoch [131/500], Train Loss: 0.4629, Train Accuracy: 75.29%\n",
      "Epoch 131/500 - Validation Loss: 0.4846, Validation Accuracy: 0.7176\n",
      "Epoch [132/500], Train Loss: 0.4712, Train Accuracy: 75.13%\n",
      "Epoch 132/500 - Validation Loss: 0.4767, Validation Accuracy: 0.7197\n",
      "Epoch [133/500], Train Loss: 0.4602, Train Accuracy: 75.40%\n",
      "Epoch 133/500 - Validation Loss: 0.4809, Validation Accuracy: 0.7261\n",
      "Epoch [134/500], Train Loss: 0.4689, Train Accuracy: 74.97%\n",
      "Epoch 134/500 - Validation Loss: 0.4902, Validation Accuracy: 0.7261\n",
      "Epoch [135/500], Train Loss: 0.4609, Train Accuracy: 75.03%\n",
      "Epoch 135/500 - Validation Loss: 0.4803, Validation Accuracy: 0.7473\n",
      "Epoch [136/500], Train Loss: 0.4535, Train Accuracy: 75.35%\n",
      "Epoch 136/500 - Validation Loss: 0.4620, Validation Accuracy: 0.7346\n",
      "Epoch [137/500], Train Loss: 0.4679, Train Accuracy: 75.61%\n",
      "Epoch 137/500 - Validation Loss: 0.4946, Validation Accuracy: 0.7389\n",
      "Epoch [138/500], Train Loss: 0.4652, Train Accuracy: 74.60%\n",
      "Epoch 138/500 - Validation Loss: 0.4937, Validation Accuracy: 0.7473\n",
      "Epoch [139/500], Train Loss: 0.4611, Train Accuracy: 75.88%\n",
      "Epoch 139/500 - Validation Loss: 0.4733, Validation Accuracy: 0.7452\n",
      "Epoch [140/500], Train Loss: 0.4580, Train Accuracy: 75.56%\n",
      "Epoch 140/500 - Validation Loss: 0.5172, Validation Accuracy: 0.7091\n",
      "Epoch [141/500], Train Loss: 0.4664, Train Accuracy: 75.19%\n",
      "Epoch 141/500 - Validation Loss: 0.4816, Validation Accuracy: 0.7516\n",
      "Epoch [142/500], Train Loss: 0.4518, Train Accuracy: 75.83%\n",
      "Epoch 142/500 - Validation Loss: 0.5013, Validation Accuracy: 0.7282\n",
      "Epoch [143/500], Train Loss: 0.4611, Train Accuracy: 74.49%\n",
      "Epoch 143/500 - Validation Loss: 0.4923, Validation Accuracy: 0.7431\n",
      "Epoch [144/500], Train Loss: 0.4704, Train Accuracy: 75.19%\n",
      "Epoch 144/500 - Validation Loss: 0.5043, Validation Accuracy: 0.7134\n",
      "Epoch [145/500], Train Loss: 0.4685, Train Accuracy: 76.36%\n",
      "Epoch 145/500 - Validation Loss: 0.4809, Validation Accuracy: 0.7346\n",
      "Epoch [146/500], Train Loss: 0.4619, Train Accuracy: 74.92%\n",
      "Epoch 146/500 - Validation Loss: 0.5387, Validation Accuracy: 0.7240\n",
      "Epoch [147/500], Train Loss: 0.4584, Train Accuracy: 76.09%\n",
      "Epoch 147/500 - Validation Loss: 0.4703, Validation Accuracy: 0.7537\n",
      "Epoch [148/500], Train Loss: 0.4693, Train Accuracy: 74.60%\n",
      "Epoch 148/500 - Validation Loss: 0.4964, Validation Accuracy: 0.7367\n",
      "Epoch [149/500], Train Loss: 0.4582, Train Accuracy: 75.77%\n",
      "Epoch 149/500 - Validation Loss: 0.4808, Validation Accuracy: 0.7473\n",
      "Epoch [150/500], Train Loss: 0.4591, Train Accuracy: 74.97%\n",
      "Epoch 150/500 - Validation Loss: 0.4747, Validation Accuracy: 0.7282\n",
      "Epoch [151/500], Train Loss: 0.4545, Train Accuracy: 76.04%\n",
      "Epoch 151/500 - Validation Loss: 0.5050, Validation Accuracy: 0.7261\n",
      "Epoch [152/500], Train Loss: 0.4645, Train Accuracy: 76.14%\n",
      "Epoch 152/500 - Validation Loss: 0.4787, Validation Accuracy: 0.7367\n",
      "Epoch [153/500], Train Loss: 0.4567, Train Accuracy: 77.05%\n",
      "Epoch 153/500 - Validation Loss: 0.4776, Validation Accuracy: 0.7410\n",
      "Epoch [154/500], Train Loss: 0.4590, Train Accuracy: 76.14%\n",
      "Epoch 154/500 - Validation Loss: 0.4839, Validation Accuracy: 0.7346\n",
      "Epoch [155/500], Train Loss: 0.4609, Train Accuracy: 75.61%\n",
      "Epoch 155/500 - Validation Loss: 0.4774, Validation Accuracy: 0.7431\n",
      "Epoch [156/500], Train Loss: 0.4626, Train Accuracy: 75.72%\n",
      "Epoch 156/500 - Validation Loss: 0.4633, Validation Accuracy: 0.7473\n",
      "Epoch [157/500], Train Loss: 0.4760, Train Accuracy: 75.19%\n",
      "Epoch 157/500 - Validation Loss: 0.4754, Validation Accuracy: 0.7219\n",
      "Epoch [158/500], Train Loss: 0.4736, Train Accuracy: 76.04%\n",
      "Epoch 158/500 - Validation Loss: 0.4961, Validation Accuracy: 0.7325\n",
      "Epoch [159/500], Train Loss: 0.4618, Train Accuracy: 75.29%\n",
      "Epoch 159/500 - Validation Loss: 0.4699, Validation Accuracy: 0.7346\n",
      "Epoch [160/500], Train Loss: 0.4537, Train Accuracy: 76.68%\n",
      "Epoch 160/500 - Validation Loss: 0.5940, Validation Accuracy: 0.7006\n",
      "Epoch [161/500], Train Loss: 0.4641, Train Accuracy: 75.72%\n",
      "Epoch 161/500 - Validation Loss: 0.6416, Validation Accuracy: 0.6773\n",
      "Epoch [162/500], Train Loss: 0.4765, Train Accuracy: 74.33%\n",
      "Epoch 162/500 - Validation Loss: 0.5548, Validation Accuracy: 0.7134\n",
      "Epoch [163/500], Train Loss: 0.4639, Train Accuracy: 76.78%\n",
      "Epoch 163/500 - Validation Loss: 0.5018, Validation Accuracy: 0.7325\n",
      "Epoch [164/500], Train Loss: 0.4779, Train Accuracy: 76.04%\n",
      "Epoch 164/500 - Validation Loss: 0.5118, Validation Accuracy: 0.7516\n",
      "Epoch [165/500], Train Loss: 0.4682, Train Accuracy: 75.56%\n",
      "Epoch 165/500 - Validation Loss: 0.4979, Validation Accuracy: 0.7367\n",
      "Epoch [166/500], Train Loss: 0.4707, Train Accuracy: 75.99%\n",
      "Epoch 166/500 - Validation Loss: 0.5217, Validation Accuracy: 0.7197\n",
      "Epoch [167/500], Train Loss: 0.4679, Train Accuracy: 76.25%\n",
      "Epoch 167/500 - Validation Loss: 0.4838, Validation Accuracy: 0.7346\n",
      "Epoch [168/500], Train Loss: 0.4650, Train Accuracy: 75.35%\n",
      "Epoch 168/500 - Validation Loss: 0.5037, Validation Accuracy: 0.7367\n",
      "Epoch [169/500], Train Loss: 0.4591, Train Accuracy: 75.24%\n",
      "Epoch 169/500 - Validation Loss: 0.4654, Validation Accuracy: 0.7389\n",
      "Epoch [170/500], Train Loss: 0.4527, Train Accuracy: 76.14%\n",
      "Epoch 170/500 - Validation Loss: 0.4738, Validation Accuracy: 0.7452\n",
      "Epoch [171/500], Train Loss: 0.4556, Train Accuracy: 76.36%\n",
      "Epoch 171/500 - Validation Loss: 0.4829, Validation Accuracy: 0.7410\n",
      "Epoch [172/500], Train Loss: 0.4613, Train Accuracy: 75.77%\n",
      "Epoch 172/500 - Validation Loss: 0.4818, Validation Accuracy: 0.7155\n",
      "Epoch [173/500], Train Loss: 0.4565, Train Accuracy: 76.14%\n",
      "Epoch 173/500 - Validation Loss: 0.5411, Validation Accuracy: 0.7282\n",
      "Epoch [174/500], Train Loss: 0.4621, Train Accuracy: 75.72%\n",
      "Epoch 174/500 - Validation Loss: 0.4705, Validation Accuracy: 0.7304\n",
      "Epoch [175/500], Train Loss: 0.4589, Train Accuracy: 75.51%\n",
      "Epoch 175/500 - Validation Loss: 0.5619, Validation Accuracy: 0.6921\n",
      "Epoch [176/500], Train Loss: 0.4636, Train Accuracy: 77.32%\n",
      "Epoch 176/500 - Validation Loss: 0.4822, Validation Accuracy: 0.7410\n",
      "Epoch [177/500], Train Loss: 0.4688, Train Accuracy: 75.83%\n",
      "Epoch 177/500 - Validation Loss: 0.4893, Validation Accuracy: 0.7389\n",
      "Epoch [178/500], Train Loss: 0.4659, Train Accuracy: 76.36%\n",
      "Epoch 178/500 - Validation Loss: 0.4850, Validation Accuracy: 0.7452\n",
      "Epoch [179/500], Train Loss: 0.4726, Train Accuracy: 76.41%\n",
      "Epoch 179/500 - Validation Loss: 0.4773, Validation Accuracy: 0.7410\n",
      "Epoch [180/500], Train Loss: 0.4686, Train Accuracy: 75.93%\n",
      "Epoch 180/500 - Validation Loss: 0.5452, Validation Accuracy: 0.7091\n",
      "Epoch [181/500], Train Loss: 0.4789, Train Accuracy: 74.76%\n",
      "Epoch 181/500 - Validation Loss: 0.4906, Validation Accuracy: 0.7389\n",
      "Epoch [182/500], Train Loss: 0.4586, Train Accuracy: 76.57%\n",
      "Epoch 182/500 - Validation Loss: 0.4843, Validation Accuracy: 0.7346\n",
      "Epoch [183/500], Train Loss: 0.4624, Train Accuracy: 75.99%\n",
      "Epoch 183/500 - Validation Loss: 0.4862, Validation Accuracy: 0.7282\n",
      "Epoch [184/500], Train Loss: 0.4760, Train Accuracy: 74.71%\n",
      "Epoch 184/500 - Validation Loss: 0.5728, Validation Accuracy: 0.7282\n",
      "Epoch [185/500], Train Loss: 0.4766, Train Accuracy: 75.93%\n",
      "Epoch 185/500 - Validation Loss: 0.4691, Validation Accuracy: 0.7431\n",
      "Epoch [186/500], Train Loss: 0.4589, Train Accuracy: 76.25%\n",
      "Epoch 186/500 - Validation Loss: 0.4870, Validation Accuracy: 0.7473\n",
      "Epoch [187/500], Train Loss: 0.4609, Train Accuracy: 76.57%\n",
      "Epoch 187/500 - Validation Loss: 0.4728, Validation Accuracy: 0.7282\n",
      "Epoch [188/500], Train Loss: 0.4652, Train Accuracy: 75.56%\n",
      "Epoch 188/500 - Validation Loss: 0.4753, Validation Accuracy: 0.7367\n",
      "Epoch [189/500], Train Loss: 0.4682, Train Accuracy: 75.35%\n",
      "Epoch 189/500 - Validation Loss: 0.5084, Validation Accuracy: 0.7155\n",
      "Epoch [190/500], Train Loss: 0.4567, Train Accuracy: 76.25%\n",
      "Epoch 190/500 - Validation Loss: 0.4635, Validation Accuracy: 0.7473\n",
      "Epoch [191/500], Train Loss: 0.4613, Train Accuracy: 75.77%\n",
      "Epoch 191/500 - Validation Loss: 0.4790, Validation Accuracy: 0.7367\n",
      "Epoch [192/500], Train Loss: 0.4577, Train Accuracy: 76.41%\n",
      "Epoch 192/500 - Validation Loss: 0.4934, Validation Accuracy: 0.7601\n",
      "Best model saved at epoch 192 with val acc: 0.7601\n",
      "Epoch [193/500], Train Loss: 0.4663, Train Accuracy: 75.29%\n",
      "Epoch 193/500 - Validation Loss: 0.4873, Validation Accuracy: 0.7431\n",
      "Epoch [194/500], Train Loss: 0.4636, Train Accuracy: 76.41%\n",
      "Epoch 194/500 - Validation Loss: 0.4853, Validation Accuracy: 0.7452\n",
      "Epoch [195/500], Train Loss: 0.4842, Train Accuracy: 74.81%\n",
      "Epoch 195/500 - Validation Loss: 0.4882, Validation Accuracy: 0.7473\n",
      "Epoch [196/500], Train Loss: 0.4661, Train Accuracy: 76.52%\n",
      "Epoch 196/500 - Validation Loss: 0.4869, Validation Accuracy: 0.7389\n",
      "Epoch [197/500], Train Loss: 0.4801, Train Accuracy: 75.29%\n",
      "Epoch 197/500 - Validation Loss: 0.4958, Validation Accuracy: 0.7367\n",
      "Epoch [198/500], Train Loss: 0.4690, Train Accuracy: 75.93%\n",
      "Epoch 198/500 - Validation Loss: 0.4889, Validation Accuracy: 0.7346\n",
      "Epoch [199/500], Train Loss: 0.4699, Train Accuracy: 76.09%\n",
      "Epoch 199/500 - Validation Loss: 0.5172, Validation Accuracy: 0.7282\n",
      "Epoch [200/500], Train Loss: 0.4663, Train Accuracy: 77.05%\n",
      "Epoch 200/500 - Validation Loss: 0.5009, Validation Accuracy: 0.7410\n",
      "Epoch [201/500], Train Loss: 0.4720, Train Accuracy: 75.72%\n",
      "Epoch 201/500 - Validation Loss: 0.5194, Validation Accuracy: 0.7113\n",
      "Epoch [202/500], Train Loss: 0.4777, Train Accuracy: 76.14%\n",
      "Epoch 202/500 - Validation Loss: 0.4766, Validation Accuracy: 0.7452\n",
      "Epoch [203/500], Train Loss: 0.4642, Train Accuracy: 76.09%\n",
      "Epoch 203/500 - Validation Loss: 0.4751, Validation Accuracy: 0.7410\n",
      "Epoch [204/500], Train Loss: 0.4680, Train Accuracy: 75.08%\n",
      "Epoch 204/500 - Validation Loss: 0.4984, Validation Accuracy: 0.7516\n",
      "Epoch [205/500], Train Loss: 0.4730, Train Accuracy: 75.45%\n",
      "Epoch 205/500 - Validation Loss: 0.4846, Validation Accuracy: 0.7197\n",
      "Epoch [206/500], Train Loss: 0.4703, Train Accuracy: 76.04%\n",
      "Epoch 206/500 - Validation Loss: 0.4771, Validation Accuracy: 0.7601\n",
      "Epoch [207/500], Train Loss: 0.4784, Train Accuracy: 75.77%\n",
      "Epoch 207/500 - Validation Loss: 0.4677, Validation Accuracy: 0.7495\n",
      "Epoch [208/500], Train Loss: 0.4845, Train Accuracy: 75.08%\n",
      "Epoch 208/500 - Validation Loss: 0.5009, Validation Accuracy: 0.7325\n",
      "Epoch [209/500], Train Loss: 0.4746, Train Accuracy: 77.10%\n",
      "Epoch 209/500 - Validation Loss: 0.5308, Validation Accuracy: 0.7261\n",
      "Epoch [210/500], Train Loss: 0.4748, Train Accuracy: 75.08%\n",
      "Epoch 210/500 - Validation Loss: 0.4762, Validation Accuracy: 0.7558\n",
      "Epoch [211/500], Train Loss: 0.4831, Train Accuracy: 73.80%\n",
      "Epoch 211/500 - Validation Loss: 0.4781, Validation Accuracy: 0.7495\n",
      "Epoch [212/500], Train Loss: 0.4765, Train Accuracy: 74.81%\n",
      "Epoch 212/500 - Validation Loss: 0.5020, Validation Accuracy: 0.7431\n",
      "Epoch [213/500], Train Loss: 0.4728, Train Accuracy: 76.57%\n",
      "Epoch 213/500 - Validation Loss: 0.4990, Validation Accuracy: 0.7389\n",
      "Epoch [214/500], Train Loss: 0.4675, Train Accuracy: 76.36%\n",
      "Epoch 214/500 - Validation Loss: 0.5409, Validation Accuracy: 0.7389\n",
      "Epoch [215/500], Train Loss: 0.4716, Train Accuracy: 74.81%\n",
      "Epoch 215/500 - Validation Loss: 0.4900, Validation Accuracy: 0.7367\n",
      "Epoch [216/500], Train Loss: 0.4719, Train Accuracy: 74.28%\n",
      "Epoch 216/500 - Validation Loss: 0.4992, Validation Accuracy: 0.7304\n",
      "Epoch [217/500], Train Loss: 0.4584, Train Accuracy: 75.72%\n",
      "Epoch 217/500 - Validation Loss: 0.4802, Validation Accuracy: 0.7049\n",
      "Epoch [218/500], Train Loss: 0.4694, Train Accuracy: 76.09%\n",
      "Epoch 218/500 - Validation Loss: 0.4673, Validation Accuracy: 0.7558\n",
      "Epoch [219/500], Train Loss: 0.4662, Train Accuracy: 77.00%\n",
      "Epoch 219/500 - Validation Loss: 0.5049, Validation Accuracy: 0.7452\n",
      "Epoch [220/500], Train Loss: 0.4779, Train Accuracy: 74.44%\n",
      "Epoch 220/500 - Validation Loss: 0.4930, Validation Accuracy: 0.7261\n",
      "Epoch [221/500], Train Loss: 0.4689, Train Accuracy: 76.36%\n",
      "Epoch 221/500 - Validation Loss: 0.5124, Validation Accuracy: 0.7197\n",
      "Epoch [222/500], Train Loss: 0.4700, Train Accuracy: 75.72%\n",
      "Epoch 222/500 - Validation Loss: 0.5156, Validation Accuracy: 0.7197\n",
      "Epoch [223/500], Train Loss: 0.4763, Train Accuracy: 75.35%\n",
      "Epoch 223/500 - Validation Loss: 0.5058, Validation Accuracy: 0.7495\n",
      "Epoch [224/500], Train Loss: 0.4742, Train Accuracy: 76.78%\n",
      "Epoch 224/500 - Validation Loss: 0.5015, Validation Accuracy: 0.7452\n",
      "Epoch [225/500], Train Loss: 0.4671, Train Accuracy: 77.26%\n",
      "Epoch 225/500 - Validation Loss: 0.4998, Validation Accuracy: 0.7452\n",
      "Epoch [226/500], Train Loss: 0.4859, Train Accuracy: 75.03%\n",
      "Epoch 226/500 - Validation Loss: 0.4939, Validation Accuracy: 0.7537\n",
      "Epoch [227/500], Train Loss: 0.4805, Train Accuracy: 75.29%\n",
      "Epoch 227/500 - Validation Loss: 0.5022, Validation Accuracy: 0.7240\n",
      "Epoch [228/500], Train Loss: 0.4618, Train Accuracy: 75.83%\n",
      "Epoch 228/500 - Validation Loss: 0.4931, Validation Accuracy: 0.7410\n",
      "Epoch [229/500], Train Loss: 0.4615, Train Accuracy: 76.73%\n",
      "Epoch 229/500 - Validation Loss: 0.4982, Validation Accuracy: 0.7325\n",
      "Epoch [230/500], Train Loss: 0.4733, Train Accuracy: 74.39%\n",
      "Epoch 230/500 - Validation Loss: 0.5299, Validation Accuracy: 0.7049\n",
      "Epoch [231/500], Train Loss: 0.4764, Train Accuracy: 74.49%\n",
      "Epoch 231/500 - Validation Loss: 0.5026, Validation Accuracy: 0.7070\n",
      "Epoch [232/500], Train Loss: 0.4825, Train Accuracy: 75.77%\n",
      "Epoch 232/500 - Validation Loss: 0.5034, Validation Accuracy: 0.7495\n",
      "Epoch [233/500], Train Loss: 0.4644, Train Accuracy: 76.78%\n",
      "Epoch 233/500 - Validation Loss: 0.5145, Validation Accuracy: 0.7537\n",
      "Epoch [234/500], Train Loss: 0.4601, Train Accuracy: 76.78%\n",
      "Epoch 234/500 - Validation Loss: 0.4953, Validation Accuracy: 0.7431\n",
      "Epoch [235/500], Train Loss: 0.4560, Train Accuracy: 76.84%\n",
      "Epoch 235/500 - Validation Loss: 0.4728, Validation Accuracy: 0.7452\n",
      "Epoch [236/500], Train Loss: 0.4694, Train Accuracy: 74.97%\n",
      "Epoch 236/500 - Validation Loss: 0.5462, Validation Accuracy: 0.7452\n",
      "Epoch [237/500], Train Loss: 0.4670, Train Accuracy: 75.45%\n",
      "Epoch 237/500 - Validation Loss: 0.5295, Validation Accuracy: 0.7452\n",
      "Epoch [238/500], Train Loss: 0.4814, Train Accuracy: 75.40%\n",
      "Epoch 238/500 - Validation Loss: 0.5519, Validation Accuracy: 0.7325\n",
      "Epoch [239/500], Train Loss: 0.4670, Train Accuracy: 74.87%\n",
      "Epoch 239/500 - Validation Loss: 0.5034, Validation Accuracy: 0.7176\n",
      "Epoch [240/500], Train Loss: 0.4786, Train Accuracy: 75.72%\n",
      "Epoch 240/500 - Validation Loss: 0.4992, Validation Accuracy: 0.7367\n",
      "Epoch [241/500], Train Loss: 0.4827, Train Accuracy: 75.13%\n",
      "Epoch 241/500 - Validation Loss: 0.4821, Validation Accuracy: 0.7240\n",
      "Epoch [242/500], Train Loss: 0.5028, Train Accuracy: 71.99%\n",
      "Epoch 242/500 - Validation Loss: 0.4954, Validation Accuracy: 0.7367\n",
      "Epoch [243/500], Train Loss: 0.4793, Train Accuracy: 74.65%\n",
      "Epoch 243/500 - Validation Loss: 0.4922, Validation Accuracy: 0.7261\n",
      "Epoch [244/500], Train Loss: 0.4711, Train Accuracy: 75.13%\n",
      "Epoch 244/500 - Validation Loss: 0.5028, Validation Accuracy: 0.7304\n",
      "Epoch [245/500], Train Loss: 0.4822, Train Accuracy: 75.24%\n",
      "Epoch 245/500 - Validation Loss: 0.4983, Validation Accuracy: 0.7346\n",
      "Epoch [246/500], Train Loss: 0.4809, Train Accuracy: 77.00%\n",
      "Epoch 246/500 - Validation Loss: 0.5511, Validation Accuracy: 0.6985\n",
      "Epoch [247/500], Train Loss: 0.4861, Train Accuracy: 74.71%\n",
      "Epoch 247/500 - Validation Loss: 0.4930, Validation Accuracy: 0.7410\n",
      "Epoch [248/500], Train Loss: 0.4805, Train Accuracy: 74.92%\n",
      "Epoch 248/500 - Validation Loss: 0.4918, Validation Accuracy: 0.7367\n",
      "Epoch [249/500], Train Loss: 0.4672, Train Accuracy: 76.68%\n",
      "Epoch 249/500 - Validation Loss: 0.4820, Validation Accuracy: 0.7261\n",
      "Epoch [250/500], Train Loss: 0.4680, Train Accuracy: 76.46%\n",
      "Epoch 250/500 - Validation Loss: 0.5332, Validation Accuracy: 0.7261\n",
      "Epoch [251/500], Train Loss: 0.4580, Train Accuracy: 76.73%\n",
      "Epoch 251/500 - Validation Loss: 0.5152, Validation Accuracy: 0.7367\n",
      "Epoch [252/500], Train Loss: 0.4772, Train Accuracy: 74.92%\n",
      "Epoch 252/500 - Validation Loss: 0.4844, Validation Accuracy: 0.7049\n",
      "Epoch [253/500], Train Loss: 0.4740, Train Accuracy: 76.46%\n",
      "Epoch 253/500 - Validation Loss: 0.5228, Validation Accuracy: 0.7537\n",
      "Epoch [254/500], Train Loss: 0.4746, Train Accuracy: 76.52%\n",
      "Epoch 254/500 - Validation Loss: 0.4791, Validation Accuracy: 0.7431\n",
      "Epoch [255/500], Train Loss: 0.4743, Train Accuracy: 76.62%\n",
      "Epoch 255/500 - Validation Loss: 0.4830, Validation Accuracy: 0.7452\n",
      "Epoch [256/500], Train Loss: 0.4613, Train Accuracy: 74.39%\n",
      "Epoch 256/500 - Validation Loss: 0.5214, Validation Accuracy: 0.7410\n",
      "Epoch [257/500], Train Loss: 0.4701, Train Accuracy: 75.51%\n",
      "Epoch 257/500 - Validation Loss: 0.4805, Validation Accuracy: 0.7367\n",
      "Epoch [258/500], Train Loss: 0.4876, Train Accuracy: 75.29%\n",
      "Epoch 258/500 - Validation Loss: 0.4949, Validation Accuracy: 0.7580\n",
      "Epoch [259/500], Train Loss: 0.4752, Train Accuracy: 75.67%\n",
      "Epoch 259/500 - Validation Loss: 0.5250, Validation Accuracy: 0.7367\n",
      "Epoch [260/500], Train Loss: 0.4907, Train Accuracy: 74.97%\n",
      "Epoch 260/500 - Validation Loss: 0.5119, Validation Accuracy: 0.7325\n",
      "Epoch [261/500], Train Loss: 0.4898, Train Accuracy: 75.24%\n",
      "Epoch 261/500 - Validation Loss: 0.4770, Validation Accuracy: 0.7431\n",
      "Epoch [262/500], Train Loss: 0.4639, Train Accuracy: 76.41%\n",
      "Epoch 262/500 - Validation Loss: 0.4878, Validation Accuracy: 0.7389\n",
      "Epoch [263/500], Train Loss: 0.4686, Train Accuracy: 74.65%\n",
      "Epoch 263/500 - Validation Loss: 0.4878, Validation Accuracy: 0.7431\n",
      "Epoch [264/500], Train Loss: 0.4673, Train Accuracy: 74.92%\n",
      "Epoch 264/500 - Validation Loss: 0.4779, Validation Accuracy: 0.7473\n",
      "Epoch [265/500], Train Loss: 0.4828, Train Accuracy: 73.43%\n",
      "Epoch 265/500 - Validation Loss: 0.5005, Validation Accuracy: 0.7219\n",
      "Epoch [266/500], Train Loss: 0.4646, Train Accuracy: 76.04%\n",
      "Epoch 266/500 - Validation Loss: 0.4852, Validation Accuracy: 0.7346\n",
      "Epoch [267/500], Train Loss: 0.4669, Train Accuracy: 74.92%\n",
      "Epoch 267/500 - Validation Loss: 0.5259, Validation Accuracy: 0.7261\n",
      "Epoch [268/500], Train Loss: 0.4609, Train Accuracy: 76.78%\n",
      "Epoch 268/500 - Validation Loss: 0.4873, Validation Accuracy: 0.7367\n",
      "Epoch [269/500], Train Loss: 0.4532, Train Accuracy: 76.46%\n",
      "Epoch 269/500 - Validation Loss: 0.5063, Validation Accuracy: 0.7410\n",
      "Epoch [270/500], Train Loss: 0.4652, Train Accuracy: 75.93%\n",
      "Epoch 270/500 - Validation Loss: 0.6429, Validation Accuracy: 0.7091\n",
      "Epoch [271/500], Train Loss: 0.4656, Train Accuracy: 76.14%\n",
      "Epoch 271/500 - Validation Loss: 0.4816, Validation Accuracy: 0.7431\n",
      "Epoch [272/500], Train Loss: 0.4656, Train Accuracy: 75.72%\n",
      "Epoch 272/500 - Validation Loss: 0.5208, Validation Accuracy: 0.7367\n",
      "Epoch [273/500], Train Loss: 0.4663, Train Accuracy: 77.48%\n",
      "Epoch 273/500 - Validation Loss: 0.4948, Validation Accuracy: 0.7558\n",
      "Epoch [274/500], Train Loss: 0.4736, Train Accuracy: 75.19%\n",
      "Epoch 274/500 - Validation Loss: 0.4651, Validation Accuracy: 0.7558\n",
      "Epoch [275/500], Train Loss: 0.4810, Train Accuracy: 75.88%\n",
      "Epoch 275/500 - Validation Loss: 0.6109, Validation Accuracy: 0.7261\n",
      "Epoch [276/500], Train Loss: 0.4603, Train Accuracy: 76.20%\n",
      "Epoch 276/500 - Validation Loss: 0.5437, Validation Accuracy: 0.7346\n",
      "Epoch [277/500], Train Loss: 0.4888, Train Accuracy: 75.40%\n",
      "Epoch 277/500 - Validation Loss: 0.4789, Validation Accuracy: 0.7389\n",
      "Epoch [278/500], Train Loss: 0.4780, Train Accuracy: 75.03%\n",
      "Epoch 278/500 - Validation Loss: 0.4930, Validation Accuracy: 0.6943\n",
      "Epoch [279/500], Train Loss: 0.4588, Train Accuracy: 76.25%\n",
      "Epoch 279/500 - Validation Loss: 0.4874, Validation Accuracy: 0.7473\n",
      "Epoch [280/500], Train Loss: 0.4877, Train Accuracy: 75.61%\n",
      "Epoch 280/500 - Validation Loss: 0.5079, Validation Accuracy: 0.7452\n",
      "Epoch [281/500], Train Loss: 0.4853, Train Accuracy: 75.77%\n",
      "Epoch 281/500 - Validation Loss: 0.4812, Validation Accuracy: 0.7431\n",
      "Epoch [282/500], Train Loss: 0.4795, Train Accuracy: 76.36%\n",
      "Epoch 282/500 - Validation Loss: 0.5817, Validation Accuracy: 0.7197\n",
      "Epoch [283/500], Train Loss: 0.4720, Train Accuracy: 75.29%\n",
      "Epoch 283/500 - Validation Loss: 0.5186, Validation Accuracy: 0.7431\n",
      "Epoch [284/500], Train Loss: 0.4892, Train Accuracy: 72.63%\n",
      "Epoch 284/500 - Validation Loss: 0.4845, Validation Accuracy: 0.7516\n",
      "Epoch [285/500], Train Loss: 0.4870, Train Accuracy: 75.83%\n",
      "Epoch 285/500 - Validation Loss: 0.5032, Validation Accuracy: 0.7367\n",
      "Epoch [286/500], Train Loss: 0.4918, Train Accuracy: 76.46%\n",
      "Epoch 286/500 - Validation Loss: 0.4865, Validation Accuracy: 0.7516\n",
      "Epoch [287/500], Train Loss: 0.4947, Train Accuracy: 74.49%\n",
      "Epoch 287/500 - Validation Loss: 0.4956, Validation Accuracy: 0.7240\n",
      "Epoch [288/500], Train Loss: 0.4753, Train Accuracy: 75.08%\n",
      "Epoch 288/500 - Validation Loss: 0.4884, Validation Accuracy: 0.7389\n",
      "Epoch [289/500], Train Loss: 0.4640, Train Accuracy: 75.99%\n",
      "Epoch 289/500 - Validation Loss: 0.5138, Validation Accuracy: 0.7134\n",
      "Epoch [290/500], Train Loss: 0.4690, Train Accuracy: 74.65%\n",
      "Epoch 290/500 - Validation Loss: 0.4969, Validation Accuracy: 0.7325\n",
      "Epoch [291/500], Train Loss: 0.4912, Train Accuracy: 73.96%\n",
      "Epoch 291/500 - Validation Loss: 0.4846, Validation Accuracy: 0.7389\n",
      "Epoch [292/500], Train Loss: 0.4905, Train Accuracy: 76.04%\n",
      "Epoch 292/500 - Validation Loss: 0.5596, Validation Accuracy: 0.6752\n",
      "Epoch [293/500], Train Loss: 0.4780, Train Accuracy: 73.32%\n",
      "Epoch 293/500 - Validation Loss: 0.4637, Validation Accuracy: 0.7367\n",
      "Epoch [294/500], Train Loss: 0.4670, Train Accuracy: 77.90%\n",
      "Epoch 294/500 - Validation Loss: 0.4945, Validation Accuracy: 0.7452\n",
      "Epoch [295/500], Train Loss: 0.4630, Train Accuracy: 77.90%\n",
      "Epoch 295/500 - Validation Loss: 0.4932, Validation Accuracy: 0.7431\n",
      "Epoch [296/500], Train Loss: 0.4751, Train Accuracy: 75.83%\n",
      "Epoch 296/500 - Validation Loss: 0.4829, Validation Accuracy: 0.7282\n",
      "Epoch [297/500], Train Loss: 0.4913, Train Accuracy: 74.60%\n",
      "Epoch 297/500 - Validation Loss: 0.4828, Validation Accuracy: 0.7389\n",
      "Epoch [298/500], Train Loss: 0.4726, Train Accuracy: 76.30%\n",
      "Epoch 298/500 - Validation Loss: 0.4903, Validation Accuracy: 0.7410\n",
      "Epoch [299/500], Train Loss: 0.4553, Train Accuracy: 76.36%\n",
      "Epoch 299/500 - Validation Loss: 0.5811, Validation Accuracy: 0.7006\n",
      "Epoch [300/500], Train Loss: 0.4668, Train Accuracy: 74.55%\n",
      "Epoch 300/500 - Validation Loss: 0.4660, Validation Accuracy: 0.7601\n",
      "Epoch [301/500], Train Loss: 0.4586, Train Accuracy: 75.45%\n",
      "Epoch 301/500 - Validation Loss: 0.4915, Validation Accuracy: 0.7537\n",
      "Epoch [302/500], Train Loss: 0.4884, Train Accuracy: 73.11%\n",
      "Epoch 302/500 - Validation Loss: 0.4951, Validation Accuracy: 0.6964\n",
      "Epoch [303/500], Train Loss: 0.4661, Train Accuracy: 73.16%\n",
      "Epoch 303/500 - Validation Loss: 0.4773, Validation Accuracy: 0.7282\n",
      "Epoch [304/500], Train Loss: 0.4886, Train Accuracy: 74.17%\n",
      "Epoch 304/500 - Validation Loss: 0.5005, Validation Accuracy: 0.7367\n",
      "Epoch [305/500], Train Loss: 0.4740, Train Accuracy: 76.36%\n",
      "Epoch 305/500 - Validation Loss: 0.4654, Validation Accuracy: 0.7346\n",
      "Epoch [306/500], Train Loss: 0.4701, Train Accuracy: 75.29%\n",
      "Epoch 306/500 - Validation Loss: 0.4632, Validation Accuracy: 0.7325\n",
      "Epoch [307/500], Train Loss: 0.5059, Train Accuracy: 74.81%\n",
      "Epoch 307/500 - Validation Loss: 0.4883, Validation Accuracy: 0.7473\n",
      "Epoch [308/500], Train Loss: 0.4601, Train Accuracy: 76.41%\n",
      "Epoch 308/500 - Validation Loss: 0.7378, Validation Accuracy: 0.6263\n",
      "Epoch [309/500], Train Loss: 0.5181, Train Accuracy: 74.17%\n",
      "Epoch 309/500 - Validation Loss: 0.5050, Validation Accuracy: 0.7367\n",
      "Epoch [310/500], Train Loss: 0.4902, Train Accuracy: 75.29%\n",
      "Epoch 310/500 - Validation Loss: 0.5211, Validation Accuracy: 0.7389\n",
      "Epoch [311/500], Train Loss: 0.4910, Train Accuracy: 74.87%\n",
      "Epoch 311/500 - Validation Loss: 0.6834, Validation Accuracy: 0.6433\n",
      "Epoch [312/500], Train Loss: 0.5019, Train Accuracy: 73.59%\n",
      "Epoch 312/500 - Validation Loss: 0.5276, Validation Accuracy: 0.7028\n",
      "Epoch [313/500], Train Loss: 0.4929, Train Accuracy: 74.81%\n",
      "Epoch 313/500 - Validation Loss: 0.4888, Validation Accuracy: 0.7537\n",
      "Epoch [314/500], Train Loss: 0.4713, Train Accuracy: 74.87%\n",
      "Epoch 314/500 - Validation Loss: 0.4778, Validation Accuracy: 0.7219\n",
      "Epoch [315/500], Train Loss: 0.4743, Train Accuracy: 75.61%\n",
      "Epoch 315/500 - Validation Loss: 0.5483, Validation Accuracy: 0.7155\n",
      "Epoch [316/500], Train Loss: 0.4914, Train Accuracy: 75.03%\n",
      "Epoch 316/500 - Validation Loss: 0.4981, Validation Accuracy: 0.7282\n",
      "Epoch [317/500], Train Loss: 0.4818, Train Accuracy: 76.09%\n",
      "Epoch 317/500 - Validation Loss: 0.4853, Validation Accuracy: 0.7389\n",
      "Epoch [318/500], Train Loss: 0.4732, Train Accuracy: 75.45%\n",
      "Epoch 318/500 - Validation Loss: 0.5716, Validation Accuracy: 0.7113\n",
      "Epoch [319/500], Train Loss: 0.4869, Train Accuracy: 75.13%\n",
      "Epoch 319/500 - Validation Loss: 0.6141, Validation Accuracy: 0.7176\n",
      "Epoch [320/500], Train Loss: 0.5059, Train Accuracy: 72.79%\n",
      "Epoch 320/500 - Validation Loss: 0.4740, Validation Accuracy: 0.7367\n",
      "Epoch [321/500], Train Loss: 0.4897, Train Accuracy: 74.76%\n",
      "Epoch 321/500 - Validation Loss: 0.5057, Validation Accuracy: 0.7028\n",
      "Epoch [322/500], Train Loss: 0.4728, Train Accuracy: 74.12%\n",
      "Epoch 322/500 - Validation Loss: 0.5560, Validation Accuracy: 0.6730\n",
      "Epoch [323/500], Train Loss: 0.4859, Train Accuracy: 74.97%\n",
      "Epoch 323/500 - Validation Loss: 0.4632, Validation Accuracy: 0.7197\n",
      "Epoch [324/500], Train Loss: 0.4577, Train Accuracy: 75.51%\n",
      "Epoch 324/500 - Validation Loss: 0.4704, Validation Accuracy: 0.7495\n",
      "Epoch [325/500], Train Loss: 0.4918, Train Accuracy: 74.97%\n",
      "Epoch 325/500 - Validation Loss: 0.4938, Validation Accuracy: 0.7325\n",
      "Epoch [326/500], Train Loss: 0.4828, Train Accuracy: 76.25%\n",
      "Epoch 326/500 - Validation Loss: 0.5018, Validation Accuracy: 0.7282\n",
      "Epoch [327/500], Train Loss: 0.4757, Train Accuracy: 75.45%\n",
      "Epoch 327/500 - Validation Loss: 0.5185, Validation Accuracy: 0.7240\n",
      "Epoch [328/500], Train Loss: 0.4755, Train Accuracy: 75.29%\n",
      "Epoch 328/500 - Validation Loss: 0.4796, Validation Accuracy: 0.7473\n",
      "Epoch [329/500], Train Loss: 0.4870, Train Accuracy: 73.16%\n",
      "Epoch 329/500 - Validation Loss: 0.5112, Validation Accuracy: 0.7261\n",
      "Epoch [330/500], Train Loss: 0.4786, Train Accuracy: 74.65%\n",
      "Epoch 330/500 - Validation Loss: 0.5254, Validation Accuracy: 0.7452\n",
      "Epoch [331/500], Train Loss: 0.5015, Train Accuracy: 72.84%\n",
      "Epoch 331/500 - Validation Loss: 0.5325, Validation Accuracy: 0.7516\n",
      "Epoch [332/500], Train Loss: 0.4795, Train Accuracy: 75.72%\n",
      "Epoch 332/500 - Validation Loss: 0.4876, Validation Accuracy: 0.7410\n",
      "Epoch [333/500], Train Loss: 0.4870, Train Accuracy: 75.77%\n",
      "Epoch 333/500 - Validation Loss: 0.5573, Validation Accuracy: 0.7304\n",
      "Epoch [334/500], Train Loss: 0.4990, Train Accuracy: 74.65%\n",
      "Epoch 334/500 - Validation Loss: 0.4907, Validation Accuracy: 0.7367\n",
      "Epoch [335/500], Train Loss: 0.4796, Train Accuracy: 75.88%\n",
      "Epoch 335/500 - Validation Loss: 0.4986, Validation Accuracy: 0.7537\n",
      "Epoch [336/500], Train Loss: 0.5271, Train Accuracy: 74.33%\n",
      "Epoch 336/500 - Validation Loss: 0.5400, Validation Accuracy: 0.7367\n",
      "Epoch [337/500], Train Loss: 0.4850, Train Accuracy: 76.62%\n",
      "Epoch 337/500 - Validation Loss: 0.5269, Validation Accuracy: 0.7155\n",
      "Epoch [338/500], Train Loss: 0.4773, Train Accuracy: 75.99%\n",
      "Epoch 338/500 - Validation Loss: 0.5966, Validation Accuracy: 0.6667\n",
      "Epoch [339/500], Train Loss: 0.4766, Train Accuracy: 75.35%\n",
      "Epoch 339/500 - Validation Loss: 0.4879, Validation Accuracy: 0.7452\n",
      "Epoch [340/500], Train Loss: 0.5091, Train Accuracy: 75.19%\n",
      "Epoch 340/500 - Validation Loss: 0.5108, Validation Accuracy: 0.7346\n",
      "Epoch [341/500], Train Loss: 0.4780, Train Accuracy: 76.57%\n",
      "Epoch 341/500 - Validation Loss: 0.4816, Validation Accuracy: 0.7558\n",
      "Epoch [342/500], Train Loss: 0.4873, Train Accuracy: 74.28%\n",
      "Epoch 342/500 - Validation Loss: 0.4856, Validation Accuracy: 0.7134\n",
      "Epoch [343/500], Train Loss: 0.4970, Train Accuracy: 74.71%\n",
      "Epoch 343/500 - Validation Loss: 0.5048, Validation Accuracy: 0.7346\n",
      "Epoch [344/500], Train Loss: 0.4973, Train Accuracy: 75.19%\n",
      "Epoch 344/500 - Validation Loss: 0.5127, Validation Accuracy: 0.7410\n",
      "Epoch [345/500], Train Loss: 0.5745, Train Accuracy: 70.18%\n",
      "Epoch 345/500 - Validation Loss: 0.5699, Validation Accuracy: 0.6603\n",
      "Epoch [346/500], Train Loss: 0.4964, Train Accuracy: 71.51%\n",
      "Epoch 346/500 - Validation Loss: 0.5366, Validation Accuracy: 0.7219\n",
      "Epoch [347/500], Train Loss: 0.5155, Train Accuracy: 74.44%\n",
      "Epoch 347/500 - Validation Loss: 0.5191, Validation Accuracy: 0.7219\n",
      "Epoch [348/500], Train Loss: 0.4935, Train Accuracy: 75.13%\n",
      "Epoch 348/500 - Validation Loss: 0.5166, Validation Accuracy: 0.7367\n",
      "Epoch [349/500], Train Loss: 0.4967, Train Accuracy: 74.60%\n",
      "Epoch 349/500 - Validation Loss: 0.4757, Validation Accuracy: 0.7452\n",
      "Epoch [350/500], Train Loss: 0.4816, Train Accuracy: 76.25%\n",
      "Epoch 350/500 - Validation Loss: 0.4985, Validation Accuracy: 0.7325\n",
      "Epoch [351/500], Train Loss: 0.4691, Train Accuracy: 74.55%\n",
      "Epoch 351/500 - Validation Loss: 0.4892, Validation Accuracy: 0.7537\n",
      "Epoch [352/500], Train Loss: 0.4695, Train Accuracy: 76.09%\n",
      "Epoch 352/500 - Validation Loss: 0.4791, Validation Accuracy: 0.7346\n",
      "Epoch [353/500], Train Loss: 0.4739, Train Accuracy: 76.25%\n",
      "Epoch 353/500 - Validation Loss: 0.5212, Validation Accuracy: 0.7537\n",
      "Epoch [354/500], Train Loss: 0.4525, Train Accuracy: 77.37%\n",
      "Epoch 354/500 - Validation Loss: 0.4862, Validation Accuracy: 0.7516\n",
      "Epoch [355/500], Train Loss: 0.4995, Train Accuracy: 74.01%\n",
      "Epoch 355/500 - Validation Loss: 0.5069, Validation Accuracy: 0.7537\n",
      "Epoch [356/500], Train Loss: 0.4922, Train Accuracy: 74.87%\n",
      "Epoch 356/500 - Validation Loss: 0.5258, Validation Accuracy: 0.7367\n",
      "Epoch [357/500], Train Loss: 0.4889, Train Accuracy: 76.14%\n",
      "Epoch 357/500 - Validation Loss: 0.6116, Validation Accuracy: 0.7304\n",
      "Epoch [358/500], Train Loss: 0.4714, Train Accuracy: 74.49%\n",
      "Epoch 358/500 - Validation Loss: 0.4995, Validation Accuracy: 0.7325\n",
      "Epoch [359/500], Train Loss: 0.4678, Train Accuracy: 75.08%\n",
      "Epoch 359/500 - Validation Loss: 0.4940, Validation Accuracy: 0.7282\n",
      "Epoch [360/500], Train Loss: 0.4731, Train Accuracy: 75.56%\n",
      "Epoch 360/500 - Validation Loss: 0.5381, Validation Accuracy: 0.7113\n",
      "Epoch [361/500], Train Loss: 0.4854, Train Accuracy: 75.61%\n",
      "Epoch 361/500 - Validation Loss: 0.4930, Validation Accuracy: 0.7410\n",
      "Epoch [362/500], Train Loss: 0.4922, Train Accuracy: 74.07%\n",
      "Epoch 362/500 - Validation Loss: 0.4882, Validation Accuracy: 0.7197\n",
      "Epoch [363/500], Train Loss: 0.4703, Train Accuracy: 76.25%\n",
      "Epoch 363/500 - Validation Loss: 0.5092, Validation Accuracy: 0.7325\n",
      "Epoch [364/500], Train Loss: 0.4767, Train Accuracy: 75.51%\n",
      "Epoch 364/500 - Validation Loss: 0.4864, Validation Accuracy: 0.7495\n",
      "Epoch [365/500], Train Loss: 0.4808, Train Accuracy: 75.93%\n",
      "Epoch 365/500 - Validation Loss: 0.4873, Validation Accuracy: 0.7473\n",
      "Epoch [366/500], Train Loss: 0.5027, Train Accuracy: 74.28%\n",
      "Epoch 366/500 - Validation Loss: 0.4823, Validation Accuracy: 0.7431\n",
      "Epoch [367/500], Train Loss: 0.4760, Train Accuracy: 75.67%\n",
      "Epoch 367/500 - Validation Loss: 0.6276, Validation Accuracy: 0.6115\n",
      "Epoch [368/500], Train Loss: 0.4742, Train Accuracy: 76.68%\n",
      "Epoch 368/500 - Validation Loss: 0.4859, Validation Accuracy: 0.7325\n",
      "Epoch [369/500], Train Loss: 0.4757, Train Accuracy: 75.61%\n",
      "Epoch 369/500 - Validation Loss: 0.5143, Validation Accuracy: 0.7261\n",
      "Epoch [370/500], Train Loss: 0.5037, Train Accuracy: 73.75%\n",
      "Epoch 370/500 - Validation Loss: 0.5114, Validation Accuracy: 0.7113\n",
      "Epoch [371/500], Train Loss: 0.4839, Train Accuracy: 75.72%\n",
      "Epoch 371/500 - Validation Loss: 0.5279, Validation Accuracy: 0.7304\n",
      "Epoch [372/500], Train Loss: 0.4735, Train Accuracy: 75.72%\n",
      "Epoch 372/500 - Validation Loss: 0.5111, Validation Accuracy: 0.7070\n",
      "Epoch [373/500], Train Loss: 0.4908, Train Accuracy: 75.61%\n",
      "Epoch 373/500 - Validation Loss: 0.5719, Validation Accuracy: 0.7197\n",
      "Epoch [374/500], Train Loss: 0.4910, Train Accuracy: 74.49%\n",
      "Epoch 374/500 - Validation Loss: 0.4987, Validation Accuracy: 0.7346\n",
      "Epoch [375/500], Train Loss: 0.4833, Train Accuracy: 76.30%\n",
      "Epoch 375/500 - Validation Loss: 0.4950, Validation Accuracy: 0.7410\n",
      "Epoch [376/500], Train Loss: 0.4775, Train Accuracy: 76.52%\n",
      "Epoch 376/500 - Validation Loss: 0.4940, Validation Accuracy: 0.7473\n",
      "Epoch [377/500], Train Loss: 0.4769, Train Accuracy: 75.77%\n",
      "Epoch 377/500 - Validation Loss: 0.5356, Validation Accuracy: 0.7346\n",
      "Epoch [378/500], Train Loss: 0.5171, Train Accuracy: 74.55%\n",
      "Epoch 378/500 - Validation Loss: 0.4913, Validation Accuracy: 0.7452\n",
      "Epoch [379/500], Train Loss: 0.4905, Train Accuracy: 74.12%\n",
      "Epoch 379/500 - Validation Loss: 0.4768, Validation Accuracy: 0.7346\n",
      "Epoch [380/500], Train Loss: 0.4764, Train Accuracy: 76.52%\n",
      "Epoch 380/500 - Validation Loss: 0.5057, Validation Accuracy: 0.7537\n",
      "Epoch [381/500], Train Loss: 0.4748, Train Accuracy: 76.46%\n",
      "Epoch 381/500 - Validation Loss: 0.5587, Validation Accuracy: 0.7219\n",
      "Epoch [382/500], Train Loss: 0.4706, Train Accuracy: 75.51%\n",
      "Epoch 382/500 - Validation Loss: 0.5211, Validation Accuracy: 0.7537\n",
      "Epoch [383/500], Train Loss: 0.4912, Train Accuracy: 73.80%\n",
      "Epoch 383/500 - Validation Loss: 0.5003, Validation Accuracy: 0.7410\n",
      "Epoch [384/500], Train Loss: 0.5113, Train Accuracy: 73.80%\n",
      "Epoch 384/500 - Validation Loss: 0.5073, Validation Accuracy: 0.7091\n",
      "Epoch [385/500], Train Loss: 0.5004, Train Accuracy: 73.75%\n",
      "Epoch 385/500 - Validation Loss: 0.5024, Validation Accuracy: 0.7282\n",
      "Epoch [386/500], Train Loss: 0.4769, Train Accuracy: 77.37%\n",
      "Epoch 386/500 - Validation Loss: 0.5217, Validation Accuracy: 0.7219\n",
      "Epoch [387/500], Train Loss: 0.4946, Train Accuracy: 74.44%\n",
      "Epoch 387/500 - Validation Loss: 0.5081, Validation Accuracy: 0.7389\n",
      "Epoch [388/500], Train Loss: 0.4715, Train Accuracy: 76.73%\n",
      "Epoch 388/500 - Validation Loss: 0.4853, Validation Accuracy: 0.7558\n",
      "Epoch [389/500], Train Loss: 0.4969, Train Accuracy: 73.86%\n",
      "Epoch 389/500 - Validation Loss: 0.5367, Validation Accuracy: 0.7325\n",
      "Epoch [390/500], Train Loss: 0.4970, Train Accuracy: 75.29%\n",
      "Epoch 390/500 - Validation Loss: 0.5107, Validation Accuracy: 0.7176\n",
      "Epoch [391/500], Train Loss: 0.4823, Train Accuracy: 76.62%\n",
      "Epoch 391/500 - Validation Loss: 0.5100, Validation Accuracy: 0.7176\n",
      "Epoch [392/500], Train Loss: 0.4930, Train Accuracy: 75.56%\n",
      "Epoch 392/500 - Validation Loss: 0.4995, Validation Accuracy: 0.7431\n",
      "Epoch [393/500], Train Loss: 0.4817, Train Accuracy: 74.87%\n",
      "Epoch 393/500 - Validation Loss: 0.4927, Validation Accuracy: 0.7134\n",
      "Epoch [394/500], Train Loss: 0.4847, Train Accuracy: 74.71%\n",
      "Epoch 394/500 - Validation Loss: 0.5022, Validation Accuracy: 0.7367\n",
      "Epoch [395/500], Train Loss: 0.4856, Train Accuracy: 75.51%\n",
      "Epoch 395/500 - Validation Loss: 0.4820, Validation Accuracy: 0.7431\n",
      "Epoch [396/500], Train Loss: 0.4772, Train Accuracy: 75.61%\n",
      "Epoch 396/500 - Validation Loss: 0.5091, Validation Accuracy: 0.7134\n",
      "Epoch [397/500], Train Loss: 0.4619, Train Accuracy: 76.73%\n",
      "Epoch 397/500 - Validation Loss: 0.5243, Validation Accuracy: 0.7325\n",
      "Epoch [398/500], Train Loss: 0.4817, Train Accuracy: 74.60%\n",
      "Epoch 398/500 - Validation Loss: 0.5143, Validation Accuracy: 0.6943\n",
      "Epoch [399/500], Train Loss: 0.4800, Train Accuracy: 76.25%\n",
      "Epoch 399/500 - Validation Loss: 0.4915, Validation Accuracy: 0.7389\n",
      "Epoch [400/500], Train Loss: 0.4750, Train Accuracy: 76.78%\n",
      "Epoch 400/500 - Validation Loss: 0.4897, Validation Accuracy: 0.7580\n",
      "Epoch [401/500], Train Loss: 0.4659, Train Accuracy: 76.57%\n",
      "Epoch 401/500 - Validation Loss: 0.4622, Validation Accuracy: 0.7516\n",
      "Epoch [402/500], Train Loss: 0.4594, Train Accuracy: 76.57%\n",
      "Epoch 402/500 - Validation Loss: 0.4963, Validation Accuracy: 0.7240\n",
      "Epoch [403/500], Train Loss: 0.4652, Train Accuracy: 75.13%\n",
      "Epoch 403/500 - Validation Loss: 0.4636, Validation Accuracy: 0.7643\n",
      "Best model saved at epoch 403 with val acc: 0.7643\n",
      "Epoch [404/500], Train Loss: 0.4563, Train Accuracy: 75.77%\n",
      "Epoch 404/500 - Validation Loss: 0.4864, Validation Accuracy: 0.7389\n",
      "Epoch [405/500], Train Loss: 0.4524, Train Accuracy: 77.21%\n",
      "Epoch 405/500 - Validation Loss: 0.4829, Validation Accuracy: 0.7325\n",
      "Epoch [406/500], Train Loss: 0.4754, Train Accuracy: 75.13%\n",
      "Epoch 406/500 - Validation Loss: 0.4803, Validation Accuracy: 0.7622\n",
      "Epoch [407/500], Train Loss: 0.4662, Train Accuracy: 76.46%\n",
      "Epoch 407/500 - Validation Loss: 0.4857, Validation Accuracy: 0.7473\n",
      "Epoch [408/500], Train Loss: 0.4864, Train Accuracy: 75.99%\n",
      "Epoch 408/500 - Validation Loss: 0.6461, Validation Accuracy: 0.7113\n",
      "Epoch [409/500], Train Loss: 0.5267, Train Accuracy: 74.65%\n",
      "Epoch 409/500 - Validation Loss: 0.5093, Validation Accuracy: 0.7325\n",
      "Epoch [410/500], Train Loss: 0.4566, Train Accuracy: 76.68%\n",
      "Epoch 410/500 - Validation Loss: 0.4910, Validation Accuracy: 0.7580\n",
      "Epoch [411/500], Train Loss: 0.4775, Train Accuracy: 75.29%\n",
      "Epoch 411/500 - Validation Loss: 0.4678, Validation Accuracy: 0.7558\n",
      "Epoch [412/500], Train Loss: 0.4703, Train Accuracy: 76.14%\n",
      "Epoch 412/500 - Validation Loss: 0.4818, Validation Accuracy: 0.7431\n",
      "Epoch [413/500], Train Loss: 0.4726, Train Accuracy: 75.13%\n",
      "Epoch 413/500 - Validation Loss: 0.4844, Validation Accuracy: 0.7197\n",
      "Epoch [414/500], Train Loss: 0.4873, Train Accuracy: 73.43%\n",
      "Epoch 414/500 - Validation Loss: 0.6353, Validation Accuracy: 0.7325\n",
      "Epoch [415/500], Train Loss: 0.4599, Train Accuracy: 76.41%\n",
      "Epoch 415/500 - Validation Loss: 0.5767, Validation Accuracy: 0.6985\n",
      "Epoch [416/500], Train Loss: 0.4826, Train Accuracy: 74.92%\n",
      "Epoch 416/500 - Validation Loss: 0.4950, Validation Accuracy: 0.7304\n",
      "Epoch [417/500], Train Loss: 0.5176, Train Accuracy: 74.87%\n",
      "Epoch 417/500 - Validation Loss: 0.5063, Validation Accuracy: 0.7346\n",
      "Epoch [418/500], Train Loss: 0.4903, Train Accuracy: 76.41%\n",
      "Epoch 418/500 - Validation Loss: 0.5381, Validation Accuracy: 0.7346\n",
      "Epoch [419/500], Train Loss: 0.4665, Train Accuracy: 77.10%\n",
      "Epoch 419/500 - Validation Loss: 0.5613, Validation Accuracy: 0.7240\n",
      "Epoch [420/500], Train Loss: 0.4934, Train Accuracy: 74.92%\n",
      "Epoch 420/500 - Validation Loss: 0.5172, Validation Accuracy: 0.7389\n",
      "Epoch [421/500], Train Loss: 0.4783, Train Accuracy: 76.25%\n",
      "Epoch 421/500 - Validation Loss: 0.5051, Validation Accuracy: 0.7410\n",
      "Epoch [422/500], Train Loss: 0.4768, Train Accuracy: 75.61%\n",
      "Epoch 422/500 - Validation Loss: 0.4927, Validation Accuracy: 0.7346\n",
      "Epoch [423/500], Train Loss: 0.4799, Train Accuracy: 77.21%\n",
      "Epoch 423/500 - Validation Loss: 0.5109, Validation Accuracy: 0.7219\n",
      "Epoch [424/500], Train Loss: 0.4817, Train Accuracy: 75.93%\n",
      "Epoch 424/500 - Validation Loss: 0.4958, Validation Accuracy: 0.7389\n",
      "Epoch [425/500], Train Loss: 0.4788, Train Accuracy: 74.01%\n",
      "Epoch 425/500 - Validation Loss: 0.4729, Validation Accuracy: 0.7452\n",
      "Epoch [426/500], Train Loss: 0.4781, Train Accuracy: 74.65%\n",
      "Epoch 426/500 - Validation Loss: 0.4965, Validation Accuracy: 0.7537\n",
      "Epoch [427/500], Train Loss: 0.4720, Train Accuracy: 75.40%\n",
      "Epoch 427/500 - Validation Loss: 0.5331, Validation Accuracy: 0.7261\n",
      "Epoch [428/500], Train Loss: 0.4806, Train Accuracy: 76.20%\n",
      "Epoch 428/500 - Validation Loss: 0.5475, Validation Accuracy: 0.7049\n",
      "Epoch [429/500], Train Loss: 0.4744, Train Accuracy: 76.30%\n",
      "Epoch 429/500 - Validation Loss: 0.5004, Validation Accuracy: 0.7346\n",
      "Epoch [430/500], Train Loss: 0.4846, Train Accuracy: 75.99%\n",
      "Epoch 430/500 - Validation Loss: 0.5003, Validation Accuracy: 0.7473\n",
      "Epoch [431/500], Train Loss: 0.4792, Train Accuracy: 76.09%\n",
      "Epoch 431/500 - Validation Loss: 0.4844, Validation Accuracy: 0.7389\n",
      "Epoch [432/500], Train Loss: 0.4686, Train Accuracy: 76.25%\n",
      "Epoch 432/500 - Validation Loss: 0.4833, Validation Accuracy: 0.7240\n",
      "Epoch [433/500], Train Loss: 0.4968, Train Accuracy: 74.44%\n",
      "Epoch 433/500 - Validation Loss: 0.4957, Validation Accuracy: 0.7261\n",
      "Epoch [434/500], Train Loss: 0.4746, Train Accuracy: 74.12%\n",
      "Epoch 434/500 - Validation Loss: 0.5023, Validation Accuracy: 0.7410\n",
      "Epoch [435/500], Train Loss: 0.4876, Train Accuracy: 75.99%\n",
      "Epoch 435/500 - Validation Loss: 0.4880, Validation Accuracy: 0.7410\n",
      "Epoch [436/500], Train Loss: 0.4717, Train Accuracy: 76.52%\n",
      "Epoch 436/500 - Validation Loss: 0.4915, Validation Accuracy: 0.7410\n",
      "Epoch [437/500], Train Loss: 0.4715, Train Accuracy: 77.05%\n",
      "Epoch 437/500 - Validation Loss: 0.4887, Validation Accuracy: 0.7367\n",
      "Epoch [438/500], Train Loss: 0.4821, Train Accuracy: 75.67%\n",
      "Epoch 438/500 - Validation Loss: 0.5314, Validation Accuracy: 0.7006\n",
      "Epoch [439/500], Train Loss: 0.4785, Train Accuracy: 74.60%\n",
      "Epoch 439/500 - Validation Loss: 0.6094, Validation Accuracy: 0.7070\n",
      "Epoch [440/500], Train Loss: 0.4868, Train Accuracy: 73.80%\n",
      "Epoch 440/500 - Validation Loss: 0.4839, Validation Accuracy: 0.7452\n",
      "Epoch [441/500], Train Loss: 0.4710, Train Accuracy: 75.56%\n",
      "Epoch 441/500 - Validation Loss: 0.5193, Validation Accuracy: 0.7113\n",
      "Epoch [442/500], Train Loss: 0.4967, Train Accuracy: 75.40%\n",
      "Epoch 442/500 - Validation Loss: 0.5323, Validation Accuracy: 0.7431\n",
      "Epoch [443/500], Train Loss: 0.5142, Train Accuracy: 74.28%\n",
      "Epoch 443/500 - Validation Loss: 0.5324, Validation Accuracy: 0.7304\n",
      "Epoch [444/500], Train Loss: 0.5038, Train Accuracy: 76.73%\n",
      "Epoch 444/500 - Validation Loss: 0.4974, Validation Accuracy: 0.7580\n",
      "Epoch [445/500], Train Loss: 0.4884, Train Accuracy: 76.30%\n",
      "Epoch 445/500 - Validation Loss: 0.5606, Validation Accuracy: 0.7028\n",
      "Epoch [446/500], Train Loss: 0.4938, Train Accuracy: 75.61%\n",
      "Epoch 446/500 - Validation Loss: 0.5159, Validation Accuracy: 0.6879\n",
      "Epoch [447/500], Train Loss: 0.5083, Train Accuracy: 74.28%\n",
      "Epoch 447/500 - Validation Loss: 0.5616, Validation Accuracy: 0.7304\n",
      "Epoch [448/500], Train Loss: 0.5121, Train Accuracy: 75.13%\n",
      "Epoch 448/500 - Validation Loss: 0.5097, Validation Accuracy: 0.7410\n",
      "Epoch [449/500], Train Loss: 0.4845, Train Accuracy: 74.81%\n",
      "Epoch 449/500 - Validation Loss: 0.4913, Validation Accuracy: 0.7346\n",
      "Epoch [450/500], Train Loss: 0.4862, Train Accuracy: 75.77%\n",
      "Epoch 450/500 - Validation Loss: 0.4900, Validation Accuracy: 0.7473\n",
      "Epoch [451/500], Train Loss: 0.4650, Train Accuracy: 76.68%\n",
      "Epoch 451/500 - Validation Loss: 0.4975, Validation Accuracy: 0.7304\n",
      "Epoch [452/500], Train Loss: 0.4721, Train Accuracy: 76.46%\n",
      "Epoch 452/500 - Validation Loss: 0.4799, Validation Accuracy: 0.7091\n",
      "Epoch [453/500], Train Loss: 0.4619, Train Accuracy: 73.75%\n",
      "Epoch 453/500 - Validation Loss: 0.4906, Validation Accuracy: 0.7261\n",
      "Epoch [454/500], Train Loss: 0.4665, Train Accuracy: 74.23%\n",
      "Epoch 454/500 - Validation Loss: 0.4973, Validation Accuracy: 0.7325\n",
      "Epoch [455/500], Train Loss: 0.4598, Train Accuracy: 76.57%\n",
      "Epoch 455/500 - Validation Loss: 0.5341, Validation Accuracy: 0.7304\n",
      "Epoch [456/500], Train Loss: 0.4772, Train Accuracy: 74.12%\n",
      "Epoch 456/500 - Validation Loss: 0.5207, Validation Accuracy: 0.6752\n",
      "Epoch [457/500], Train Loss: 0.4794, Train Accuracy: 74.60%\n",
      "Epoch 457/500 - Validation Loss: 0.5853, Validation Accuracy: 0.7049\n",
      "Epoch [458/500], Train Loss: 0.4750, Train Accuracy: 76.30%\n",
      "Epoch 458/500 - Validation Loss: 0.4731, Validation Accuracy: 0.7516\n",
      "Epoch [459/500], Train Loss: 0.4933, Train Accuracy: 76.04%\n",
      "Epoch 459/500 - Validation Loss: 0.7226, Validation Accuracy: 0.6858\n",
      "Epoch [460/500], Train Loss: 0.4796, Train Accuracy: 75.99%\n",
      "Epoch 460/500 - Validation Loss: 0.5150, Validation Accuracy: 0.7049\n",
      "Epoch [461/500], Train Loss: 0.4706, Train Accuracy: 75.03%\n",
      "Epoch 461/500 - Validation Loss: 0.4898, Validation Accuracy: 0.7580\n",
      "Epoch [462/500], Train Loss: 0.5192, Train Accuracy: 72.31%\n",
      "Epoch 462/500 - Validation Loss: 0.4892, Validation Accuracy: 0.7516\n",
      "Epoch [463/500], Train Loss: 0.4952, Train Accuracy: 74.76%\n",
      "Epoch 463/500 - Validation Loss: 0.5764, Validation Accuracy: 0.6645\n",
      "Epoch [464/500], Train Loss: 0.4825, Train Accuracy: 75.13%\n",
      "Epoch 464/500 - Validation Loss: 0.6296, Validation Accuracy: 0.6752\n",
      "Epoch [465/500], Train Loss: 0.5001, Train Accuracy: 75.40%\n",
      "Epoch 465/500 - Validation Loss: 0.5055, Validation Accuracy: 0.7452\n",
      "Epoch [466/500], Train Loss: 0.4975, Train Accuracy: 74.44%\n",
      "Epoch 466/500 - Validation Loss: 0.5418, Validation Accuracy: 0.7176\n",
      "Epoch [467/500], Train Loss: 0.4810, Train Accuracy: 75.67%\n",
      "Epoch 467/500 - Validation Loss: 0.5149, Validation Accuracy: 0.7473\n",
      "Epoch [468/500], Train Loss: 0.4697, Train Accuracy: 76.09%\n",
      "Epoch 468/500 - Validation Loss: 0.5305, Validation Accuracy: 0.7410\n",
      "Epoch [469/500], Train Loss: 0.4730, Train Accuracy: 75.83%\n",
      "Epoch 469/500 - Validation Loss: 0.5972, Validation Accuracy: 0.7176\n",
      "Epoch [470/500], Train Loss: 0.4699, Train Accuracy: 75.29%\n",
      "Epoch 470/500 - Validation Loss: 0.5106, Validation Accuracy: 0.7346\n",
      "Epoch [471/500], Train Loss: 0.4819, Train Accuracy: 76.25%\n",
      "Epoch 471/500 - Validation Loss: 0.5151, Validation Accuracy: 0.7452\n",
      "Epoch [472/500], Train Loss: 0.4586, Train Accuracy: 74.92%\n",
      "Epoch 472/500 - Validation Loss: 0.4914, Validation Accuracy: 0.7410\n",
      "Epoch [473/500], Train Loss: 0.4713, Train Accuracy: 76.20%\n",
      "Epoch 473/500 - Validation Loss: 0.6021, Validation Accuracy: 0.7197\n",
      "Epoch [474/500], Train Loss: 0.5183, Train Accuracy: 73.27%\n",
      "Epoch 474/500 - Validation Loss: 0.5677, Validation Accuracy: 0.6115\n",
      "Epoch [475/500], Train Loss: 0.4822, Train Accuracy: 74.07%\n",
      "Epoch 475/500 - Validation Loss: 0.4899, Validation Accuracy: 0.7049\n",
      "Epoch [476/500], Train Loss: 0.4812, Train Accuracy: 76.68%\n",
      "Epoch 476/500 - Validation Loss: 0.4868, Validation Accuracy: 0.7431\n",
      "Epoch [477/500], Train Loss: 0.4716, Train Accuracy: 75.45%\n",
      "Epoch 477/500 - Validation Loss: 0.5120, Validation Accuracy: 0.7134\n",
      "Epoch [478/500], Train Loss: 0.4882, Train Accuracy: 74.60%\n",
      "Epoch 478/500 - Validation Loss: 0.5185, Validation Accuracy: 0.7410\n",
      "Epoch [479/500], Train Loss: 0.4940, Train Accuracy: 75.77%\n",
      "Epoch 479/500 - Validation Loss: 0.5375, Validation Accuracy: 0.7325\n",
      "Epoch [480/500], Train Loss: 0.4930, Train Accuracy: 74.12%\n",
      "Epoch 480/500 - Validation Loss: 0.5187, Validation Accuracy: 0.7325\n",
      "Epoch [481/500], Train Loss: 0.4722, Train Accuracy: 75.72%\n",
      "Epoch 481/500 - Validation Loss: 0.5016, Validation Accuracy: 0.7558\n",
      "Epoch [482/500], Train Loss: 0.4667, Train Accuracy: 75.67%\n",
      "Epoch 482/500 - Validation Loss: 0.4625, Validation Accuracy: 0.7516\n",
      "Epoch [483/500], Train Loss: 0.4680, Train Accuracy: 75.13%\n",
      "Epoch 483/500 - Validation Loss: 0.6578, Validation Accuracy: 0.7155\n",
      "Epoch [484/500], Train Loss: 0.4895, Train Accuracy: 74.44%\n",
      "Epoch 484/500 - Validation Loss: 0.5234, Validation Accuracy: 0.7367\n",
      "Epoch [485/500], Train Loss: 0.4755, Train Accuracy: 75.83%\n",
      "Epoch 485/500 - Validation Loss: 0.4976, Validation Accuracy: 0.7537\n",
      "Epoch [486/500], Train Loss: 0.4912, Train Accuracy: 75.72%\n",
      "Epoch 486/500 - Validation Loss: 0.5104, Validation Accuracy: 0.7389\n",
      "Epoch [487/500], Train Loss: 0.4806, Train Accuracy: 75.83%\n",
      "Epoch 487/500 - Validation Loss: 0.5041, Validation Accuracy: 0.7304\n",
      "Epoch [488/500], Train Loss: 0.4911, Train Accuracy: 76.36%\n",
      "Epoch 488/500 - Validation Loss: 0.5037, Validation Accuracy: 0.7134\n",
      "Epoch [489/500], Train Loss: 0.4803, Train Accuracy: 73.75%\n",
      "Epoch 489/500 - Validation Loss: 0.5178, Validation Accuracy: 0.7219\n",
      "Epoch [490/500], Train Loss: 0.4855, Train Accuracy: 76.52%\n",
      "Epoch 490/500 - Validation Loss: 0.4859, Validation Accuracy: 0.7367\n",
      "Epoch [491/500], Train Loss: 0.5054, Train Accuracy: 74.17%\n",
      "Epoch 491/500 - Validation Loss: 0.5041, Validation Accuracy: 0.7452\n",
      "Epoch [492/500], Train Loss: 0.4974, Train Accuracy: 74.65%\n",
      "Epoch 492/500 - Validation Loss: 0.5144, Validation Accuracy: 0.7346\n",
      "Epoch [493/500], Train Loss: 0.4953, Train Accuracy: 75.61%\n",
      "Epoch 493/500 - Validation Loss: 0.5060, Validation Accuracy: 0.7431\n",
      "Epoch [494/500], Train Loss: 0.4807, Train Accuracy: 75.88%\n",
      "Epoch 494/500 - Validation Loss: 0.4883, Validation Accuracy: 0.7410\n",
      "Epoch [495/500], Train Loss: 0.4815, Train Accuracy: 75.93%\n",
      "Epoch 495/500 - Validation Loss: 0.4910, Validation Accuracy: 0.7495\n",
      "Epoch [496/500], Train Loss: 0.4857, Train Accuracy: 74.81%\n",
      "Epoch 496/500 - Validation Loss: 0.5224, Validation Accuracy: 0.7070\n",
      "Epoch [497/500], Train Loss: 0.4738, Train Accuracy: 76.41%\n",
      "Epoch 497/500 - Validation Loss: 0.4979, Validation Accuracy: 0.7389\n",
      "Epoch [498/500], Train Loss: 0.4899, Train Accuracy: 74.44%\n",
      "Epoch 498/500 - Validation Loss: 0.4791, Validation Accuracy: 0.7389\n",
      "Epoch [499/500], Train Loss: 0.5172, Train Accuracy: 74.07%\n",
      "Epoch 499/500 - Validation Loss: 0.5279, Validation Accuracy: 0.7240\n",
      "Epoch [500/500], Train Loss: 0.5239, Train Accuracy: 74.81%\n",
      "Epoch 500/500 - Validation Loss: 0.5249, Validation Accuracy: 0.7006\n",
      "Best validation accuracy: 0.7643\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard\n",
    "writer = SummaryWriter(log_dir='./runs/convLSTM')\n",
    "# Variables to keep track of the best validation accuracy\n",
    "best_val_acc = 0.0\n",
    "best_model_path = '7-segment_datasets/highRel_weights_z/best_model.pth'\n",
    "last_model_path = '7-segment_datasets/highRel_weights_z/last_model.pth'\n",
    "num_epochs = 500\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for keypoints, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(keypoints)  # outputs shape should be [batch_size, 1]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('outputs', outputs)\n",
    "        # Calculate accuracy\n",
    "        predicted_labels = outputs.round()  # Round the outputs to get the final predictions for binary classification\n",
    "        # print('predicted_labels',predicted_labels, 'True labels:', labels)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Calculate accuracy and loss for the epoch\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # Perform validation\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # Log loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    \n",
    "    # Print validation metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the last model weights\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "    \n",
    "    # Save the best model weights based on validation accuracy\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Best model saved at epoch {epoch+1} with val acc: {val_accuracy:.4f}')\n",
    "\n",
    "print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_389605/3239409828.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('7-segment_datasets/highRel_weights_z/best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('7-segment_datasets/highRel_weights_z/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9491525423728814\n",
      "Recall:  0.6222222222222222\n",
      "f1-score:  0.7516778523489933\n",
      "Average Loss: 0.4636344749425121, Accuracy: 0.7643312101910829\n",
      "Confusion Matrix:\n",
      " [[192   9]\n",
      " [102 168]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function for evaluating the model on the validation set\n",
    "def evaluate_with_confusion_matrix(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for keypoints, labels in val_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(keypoints)  # outputs shape should be [batch_size, 1]\n",
    "            # outputs = outputs.squeeze()\n",
    "            # labels = labels.squeeze().long()\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predicted_labels = outputs.round()  # This should be along dimension 1 for batch data\n",
    "            all_labels.append(labels.tolist()[0][0])\n",
    "            all_predictions.append(predicted_labels.tolist()[0][0])\n",
    "            \n",
    "        # Calculate average loss\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('f1-score: ', f1)\n",
    "    return avg_loss, accuracy, cm\n",
    "\n",
    "# Use the function\n",
    "avg_loss, accuracy, cm = evaluate_with_confusion_matrix(model, val_loader, criterion)\n",
    "print(f\"Average Loss: {avg_loss}, Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8706467661691543"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "175/(26+175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6592592592592592"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "178/(92+178)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/athenaai/anaconda3/envs/basketball/lib/python3.11/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n",
      "Skipping constant folding for op SequenceConstruct with multiple outputs.\n",
      "Skipping constant folding for op SequenceConstruct with multiple outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied 20 of general pattern rewrite rules.\n"
     ]
    }
   ],
   "source": [
    "# torch_model = MyModel()\n",
    "torch_input = torch.randn(1, 20, 33, 2)\n",
    "onnx_program = torch.onnx.dynamo_export(model, torch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"shot_classifier_weights_balanced/shot_best.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(\"shot_classifier_weights_balanced/shot_best.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define LSTM classifier model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate sample data\n",
    "# X_train = np.random.rand(100, 10, 50)  # 100 sequences of length 10 with 50 features each\n",
    "# y_train = np.random.randint(2, size=(100,))  # Binary labels (0 or 1)\n",
    "\n",
    "# # Convert data to PyTorch tensors\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 33*2\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loss: 0.7066145324707032. Accuracy: 46.0\n",
      " Loss: 0.646399582028389. Accuracy: 74.0\n",
      " Loss: 0.6253186723589897. Accuracy: 62.0\n",
      " Loss: 0.6004764226451517. Accuracy: 66.0\n",
      " Loss: 0.5673007851839066. Accuracy: 64.0\n",
      " Loss: 0.5666879870928824. Accuracy: 68.0\n",
      " Loss: 0.5529879846051335. Accuracy: 68.0\n",
      " Loss: 0.5546136916801333. Accuracy: 70.0\n",
      " Loss: 0.5598513038922102. Accuracy: 68.0\n",
      " Loss: 0.5483754526730626. Accuracy: 70.0\n",
      " Loss: 0.5462060511251912. Accuracy: 68.0\n",
      " Loss: 0.5345488362945616. Accuracy: 74.0\n",
      " Loss: 0.6796253682486713. Accuracy: 60.0\n",
      " Loss: 0.5924330477602779. Accuracy: 68.0\n",
      " Loss: 0.5473819950129837. Accuracy: 70.0\n",
      " Loss: 0.5350996433291584. Accuracy: 72.0\n",
      " Loss: 0.5204374854825438. Accuracy: 74.0\n",
      " Loss: 0.6699774176906794. Accuracy: 68.0\n",
      " Loss: 0.5793360046669841. Accuracy: 68.0\n",
      " Loss: 0.5573308748006821. Accuracy: 66.0\n",
      " Loss: 0.5372575928643346. Accuracy: 76.0\n",
      " Loss: 0.5082164804823697. Accuracy: 74.0\n",
      " Loss: 0.5007441286928952. Accuracy: 74.0\n",
      " Loss: 0.5154678407032043. Accuracy: 74.0\n",
      " Loss: 0.491929565994069. Accuracy: 74.0\n",
      " Loss: 0.49011919064447285. Accuracy: 74.0\n",
      " Loss: 0.47868592170998453. Accuracy: 76.0\n",
      " Loss: 0.5187575069256127. Accuracy: 74.0\n",
      " Loss: 0.5456528769340366. Accuracy: 74.0\n",
      " Loss: 0.5380331917479634. Accuracy: 70.0\n",
      " Loss: 0.5138354414235801. Accuracy: 74.0\n",
      " Loss: 0.4944084964180365. Accuracy: 74.0\n",
      " Loss: 0.47105341233313086. Accuracy: 76.0\n",
      " Loss: 0.48384046828374266. Accuracy: 76.0\n",
      " Loss: 0.4540169785055332. Accuracy: 80.0\n",
      " Loss: 0.5137415173673071. Accuracy: 76.0\n",
      " Loss: 0.4838935429346748. Accuracy: 76.0\n",
      " Loss: 0.4676077505806461. Accuracy: 80.0\n",
      " Loss: 0.45681308601633647. Accuracy: 76.0\n",
      " Loss: 0.4499690533522516. Accuracy: 76.0\n",
      " Loss: 0.4467023221711861. Accuracy: 78.0\n",
      " Loss: 0.549769673742121. Accuracy: 72.0\n",
      " Loss: 0.5154549832618796. Accuracy: 72.0\n",
      " Loss: 0.5043760207341984. Accuracy: 76.0\n",
      " Loss: 0.5004669849167113. Accuracy: 74.0\n",
      " Loss: 0.4847272921912372. Accuracy: 70.0\n",
      " Loss: 0.49492500294116326. Accuracy: 72.0\n",
      " Loss: 0.48430708873900585. Accuracy: 74.0\n",
      " Loss: 0.45211762200458905. Accuracy: 78.0\n",
      " Loss: 0.45312213178141975. Accuracy: 76.0\n",
      " Loss: 0.4381473625183571. Accuracy: 78.0\n",
      " Loss: 0.43301255904545544. Accuracy: 78.0\n",
      " Loss: 0.45394385475781746. Accuracy: 78.0\n",
      " Loss: 0.4340535268053645. Accuracy: 80.0\n",
      " Loss: 0.4561586710609845. Accuracy: 78.0\n",
      " Loss: 0.4437955023255199. Accuracy: 78.0\n",
      " Loss: 0.42469797885336447. Accuracy: 80.0\n",
      " Loss: 0.48726523157383783. Accuracy: 82.0\n",
      " Loss: 0.48128486077970595. Accuracy: 76.0\n",
      " Loss: 0.4683176356588956. Accuracy: 76.0\n",
      " Loss: 0.4470579570170958. Accuracy: 80.0\n",
      " Loss: 0.42680899807019157. Accuracy: 78.0\n",
      " Loss: 0.4236650748785178. Accuracy: 84.0\n",
      " Loss: 0.42549551185191375. Accuracy: 76.0\n",
      " Loss: 0.4524866472305439. Accuracy: 78.0\n",
      " Loss: 0.4174409980108612. Accuracy: 84.0\n",
      " Loss: 0.40993168925881035. Accuracy: 82.0\n",
      " Loss: 0.3917220845284464. Accuracy: 78.0\n",
      " Loss: 0.5313026110101782. Accuracy: 74.0\n",
      " Loss: 0.468573915862944. Accuracy: 76.0\n",
      " Loss: 0.4230941023633932. Accuracy: 78.0\n",
      " Loss: 0.41588298163216675. Accuracy: 84.0\n",
      " Loss: 0.3955786881025415. Accuracy: 84.0\n",
      " Loss: 0.44131602150941035. Accuracy: 84.0\n",
      " Loss: 0.4620744842060958. Accuracy: 76.0\n",
      " Loss: 0.4292114995155134. Accuracy: 82.0\n",
      " Loss: 0.4428113011023379. Accuracy: 80.0\n",
      " Loss: 0.41695955002403934. Accuracy: 82.0\n",
      " Loss: 0.45192942066758407. Accuracy: 80.0\n",
      " Loss: 0.44178273196870577. Accuracy: 80.0\n",
      " Loss: 0.40325913195556495. Accuracy: 84.0\n",
      " Loss: 0.4135068410594249. Accuracy: 80.0\n",
      " Loss: 0.40264242402190575. Accuracy: 84.0\n",
      " Loss: 0.407816702817363. Accuracy: 82.0\n",
      " Loss: 0.41132532719377196. Accuracy: 82.0\n",
      " Loss: 0.40155226649723774. Accuracy: 84.0\n",
      " Loss: 0.40159410825246594. Accuracy: 84.0\n",
      " Loss: 0.41556128794356484. Accuracy: 80.0\n",
      " Loss: 0.3979645054078355. Accuracy: 84.0\n",
      " Loss: 0.3866976532696935. Accuracy: 84.0\n",
      " Loss: 0.3895373556814593. Accuracy: 84.0\n",
      " Loss: 0.43539827143526055. Accuracy: 80.0\n",
      " Loss: 0.3953760405266803. Accuracy: 84.0\n",
      " Loss: 0.38476892631026205. Accuracy: 84.0\n",
      " Loss: 0.38738049846588185. Accuracy: 84.0\n",
      " Loss: 0.42032686253989593. Accuracy: 80.0\n",
      " Loss: 0.39721826980992775. Accuracy: 84.0\n",
      " Loss: 0.4596839774270757. Accuracy: 80.0\n",
      " Loss: 0.4214275606557203. Accuracy: 84.0\n",
      " Loss: 0.40881003616086675. Accuracy: 82.0\n",
      " Loss: 0.4479318383156351. Accuracy: 78.0\n",
      " Loss: 0.39760638457468306. Accuracy: 84.0\n",
      " Loss: 0.4193862372239528. Accuracy: 80.0\n",
      " Loss: 0.37457676192621875. Accuracy: 80.0\n",
      " Loss: 0.4151988805589644. Accuracy: 84.0\n",
      " Loss: 0.38920476628798495. Accuracy: 84.0\n",
      " Loss: 0.37930040366663886. Accuracy: 84.0\n",
      " Loss: 0.3793865845665641. Accuracy: 84.0\n",
      " Loss: 0.3700385359554275. Accuracy: 82.0\n",
      " Loss: 0.3803651960460411. Accuracy: 84.0\n",
      " Loss: 0.37531017476245326. Accuracy: 84.0\n",
      " Loss: 0.3680979106122504. Accuracy: 84.0\n",
      " Loss: 0.36545489351690774. Accuracy: 84.0\n",
      " Loss: 0.3742794814778426. Accuracy: 84.0\n",
      " Loss: 0.3708180990998153. Accuracy: 84.0\n",
      " Loss: 0.3760576404196945. Accuracy: 84.0\n",
      " Loss: 0.36975815346367197. Accuracy: 84.0\n",
      " Loss: 0.3704696704093658. Accuracy: 84.0\n",
      " Loss: 0.37809742774876215. Accuracy: 84.0\n",
      " Loss: 0.3772053028448136. Accuracy: 84.0\n",
      " Loss: 0.4269064819032519. Accuracy: 78.0\n",
      " Loss: 0.4104745221691155. Accuracy: 80.0\n",
      " Loss: 0.3980457068645228. Accuracy: 84.0\n",
      " Loss: 0.3718966807174183. Accuracy: 84.0\n",
      " Loss: 0.3728462723702796. Accuracy: 84.0\n",
      " Loss: 0.36996979034002836. Accuracy: 84.0\n",
      " Loss: 0.3715433674649012. Accuracy: 84.0\n",
      " Loss: 0.3615293633222564. Accuracy: 84.0\n",
      " Loss: 0.3647250885560698. Accuracy: 84.0\n",
      " Loss: 0.36043445901674204. Accuracy: 84.0\n",
      " Loss: 0.37536946139362953. Accuracy: 84.0\n",
      " Loss: 0.3687464794996094. Accuracy: 84.0\n",
      " Loss: 0.36793246095776794. Accuracy: 84.0\n",
      " Loss: 0.36846853517798533. Accuracy: 82.0\n",
      " Loss: 0.5037462927079878. Accuracy: 74.0\n",
      " Loss: 0.4799498023791239. Accuracy: 78.0\n",
      " Loss: 0.4639747517276555. Accuracy: 68.0\n",
      " Loss: 0.46637213656678794. Accuracy: 76.0\n",
      " Loss: 0.4571738139574882. Accuracy: 80.0\n",
      " Loss: 0.42955035681894516. Accuracy: 72.0\n",
      " Loss: 0.4225368636887288. Accuracy: 82.0\n",
      " Loss: 0.42106073784758336. Accuracy: 80.0\n",
      " Loss: 0.40782172978215386. Accuracy: 82.0\n",
      " Loss: 0.41368174845760225. Accuracy: 82.0\n",
      " Loss: 0.4322566197867854. Accuracy: 82.0\n",
      " Loss: 0.4442172317806399. Accuracy: 78.0\n",
      " Loss: 0.42712204313720575. Accuracy: 82.0\n",
      " Loss: 0.42434190505504377. Accuracy: 82.0\n",
      " Loss: 0.40468572386249435. Accuracy: 82.0\n",
      " Loss: 0.3748758982296567. Accuracy: 82.0\n",
      " Loss: 0.4522734271077206. Accuracy: 78.0\n",
      " Loss: 0.41044052252138497. Accuracy: 82.0\n",
      " Loss: 0.3919880606084189. Accuracy: 82.0\n",
      " Loss: 0.3645780110984924. Accuracy: 82.0\n",
      " Loss: 0.4448632637548144. Accuracy: 80.0\n",
      " Loss: 0.44807078256621025. Accuracy: 78.0\n",
      " Loss: 0.43195798965956783. Accuracy: 78.0\n",
      " Loss: 0.4197839393984759. Accuracy: 82.0\n",
      " Loss: 0.4037869389905245. Accuracy: 82.0\n",
      " Loss: 0.393424141490832. Accuracy: 82.0\n",
      " Loss: 0.3998468673969182. Accuracy: 82.0\n",
      " Loss: 0.38264905664858817. Accuracy: 76.0\n",
      " Loss: 0.4039339827960066. Accuracy: 82.0\n",
      " Loss: 0.3800491862276976. Accuracy: 82.0\n",
      " Loss: 0.3930494706187164. Accuracy: 82.0\n",
      " Loss: 0.39449115577433985. Accuracy: 84.0\n",
      " Loss: 0.3894964782048555. Accuracy: 82.0\n",
      " Loss: 0.3927644869155847. Accuracy: 84.0\n",
      " Loss: 0.3867649926772356. Accuracy: 84.0\n",
      " Loss: 0.40879352226089394. Accuracy: 82.0\n",
      " Loss: 0.3915973423200558. Accuracy: 82.0\n",
      " Loss: 0.37340939288656955. Accuracy: 84.0\n",
      " Loss: 0.3897935276613862. Accuracy: 82.0\n",
      " Loss: 0.44899055759255135. Accuracy: 82.0\n",
      " Loss: 0.4340575645300851. Accuracy: 80.0\n",
      " Loss: 0.3931165436276933. Accuracy: 82.0\n",
      " Loss: 0.3891735098845311. Accuracy: 84.0\n",
      " Loss: 0.39200953156170726. Accuracy: 82.0\n",
      " Loss: 0.38134421821410797. Accuracy: 84.0\n",
      " Loss: 0.37139519217300404. Accuracy: 84.0\n",
      " Loss: 0.37872719352053535. Accuracy: 84.0\n",
      " Loss: 0.3704559535390763. Accuracy: 84.0\n",
      " Loss: 0.375372443143533. Accuracy: 82.0\n",
      " Loss: 0.49380582963407504. Accuracy: 76.0\n",
      " Loss: 0.4481559049594216. Accuracy: 78.0\n",
      " Loss: 0.4192502280764893. Accuracy: 82.0\n",
      " Loss: 0.3996598206770432. Accuracy: 82.0\n",
      " Loss: 0.383526624376791. Accuracy: 82.0\n",
      " Loss: 0.3937444014297944. Accuracy: 84.0\n",
      " Loss: 0.3738828547547382. Accuracy: 82.0\n",
      " Loss: 0.3674688363639143. Accuracy: 84.0\n",
      " Loss: 0.3793345307924574. Accuracy: 84.0\n",
      " Loss: 0.3726789742004803. Accuracy: 84.0\n",
      " Loss: 0.36724269434500456. Accuracy: 84.0\n",
      " Loss: 0.3792574047850394. Accuracy: 84.0\n",
      " Loss: 0.39333001332466666. Accuracy: 82.0\n",
      " Loss: 0.3665217579421187. Accuracy: 84.0\n",
      " Loss: 0.3928975351905865. Accuracy: 84.0\n",
      " Loss: 0.37129082939622093. Accuracy: 84.0\n",
      " Loss: 0.36149948366350143. Accuracy: 84.0\n",
      " Loss: 0.35409602921945404. Accuracy: 84.0\n",
      " Loss: 0.35932688440632776. Accuracy: 84.0\n",
      " Loss: 0.36173097175846125. Accuracy: 84.0\n",
      " Loss: 0.3615848972808817. Accuracy: 84.0\n",
      " Loss: 0.36402242003490753. Accuracy: 84.0\n",
      " Loss: 0.35765417798891574. Accuracy: 84.0\n",
      " Loss: 0.35258309773918883. Accuracy: 84.0\n",
      " Loss: 0.36193096761753624. Accuracy: 84.0\n",
      " Loss: 0.3541015766606506. Accuracy: 84.0\n",
      " Loss: 0.3481643599681047. Accuracy: 84.0\n",
      " Loss: 0.3502195551664408. Accuracy: 84.0\n",
      " Loss: 0.36323767441982. Accuracy: 84.0\n",
      " Loss: 0.3603050285758945. Accuracy: 84.0\n",
      " Loss: 0.3514341035405505. Accuracy: 84.0\n",
      " Loss: 0.35809033022196674. Accuracy: 84.0\n",
      " Loss: 0.36434317481431205. Accuracy: 84.0\n",
      " Loss: 0.3643287831046109. Accuracy: 84.0\n",
      " Loss: 0.35770691132112914. Accuracy: 84.0\n",
      " Loss: 0.3590440855343695. Accuracy: 84.0\n",
      " Loss: 0.41056559257925074. Accuracy: 82.0\n",
      " Loss: 0.3764590762981152. Accuracy: 84.0\n",
      " Loss: 0.35563294017798397. Accuracy: 84.0\n",
      " Loss: 0.36148073609507264. Accuracy: 84.0\n",
      " Loss: 0.37475329485464953. Accuracy: 82.0\n",
      " Loss: 0.4913797391256594. Accuracy: 80.0\n",
      " Loss: 0.40659384868162307. Accuracy: 82.0\n",
      " Loss: 0.3973059046132039. Accuracy: 82.0\n",
      " Loss: 0.3946212200910668. Accuracy: 84.0\n",
      " Loss: 0.407928242336593. Accuracy: 82.0\n",
      " Loss: 0.3911394713305981. Accuracy: 80.0\n",
      " Loss: 0.36799939407003424. Accuracy: 84.0\n",
      " Loss: 0.3671415569042756. Accuracy: 84.0\n",
      " Loss: 0.35718629470120505. Accuracy: 84.0\n",
      " Loss: 0.36857746891002535. Accuracy: 84.0\n",
      " Loss: 0.3496448707508262. Accuracy: 84.0\n",
      " Loss: 0.36372862593380434. Accuracy: 84.0\n",
      " Loss: 0.36372777687392954. Accuracy: 84.0\n",
      " Loss: 0.3547714433648389. Accuracy: 84.0\n",
      " Loss: 0.3514246182336501. Accuracy: 84.0\n",
      " Loss: 0.35714382964182734. Accuracy: 84.0\n",
      " Loss: 0.3480242860709041. Accuracy: 84.0\n",
      " Loss: 0.3512975564447743. Accuracy: 84.0\n",
      " Loss: 0.35073696712840957. Accuracy: 84.0\n",
      " Loss: 0.34733456450291667. Accuracy: 84.0\n",
      " Loss: 0.349660110310665. Accuracy: 84.0\n",
      " Loss: 0.3654020453463181. Accuracy: 84.0\n",
      " Loss: 0.34817021538532117. Accuracy: 84.0\n",
      " Loss: 0.3516530360100614. Accuracy: 84.0\n",
      " Loss: 0.35241488813403976. Accuracy: 84.0\n",
      " Loss: 0.3468438511494378. Accuracy: 84.0\n",
      " Loss: 0.3470488383698239. Accuracy: 84.0\n",
      " Loss: 0.35343672528277237. Accuracy: 84.0\n",
      " Loss: 0.356680310648785. Accuracy: 84.0\n",
      " Loss: 0.39510719246884946. Accuracy: 82.0\n",
      " Loss: 0.40778715721277875. Accuracy: 82.0\n",
      " Loss: 0.40042488344677624. Accuracy: 82.0\n",
      " Loss: 0.38521680830222976. Accuracy: 82.0\n",
      " Loss: 0.38852179051425995. Accuracy: 82.0\n",
      " Loss: 0.38545718232534454. Accuracy: 82.0\n",
      " Loss: 0.39243469465051023. Accuracy: 82.0\n",
      " Loss: 0.384252788428239. Accuracy: 82.0\n",
      " Loss: 0.38044875579049403. Accuracy: 82.0\n",
      " Loss: 0.37942651608464073. Accuracy: 82.0\n",
      " Loss: 0.3888573416645613. Accuracy: 82.0\n",
      " Loss: 0.38935034516376615. Accuracy: 82.0\n",
      " Loss: 0.38984518163525533. Accuracy: 82.0\n",
      " Loss: 0.3826135050556786. Accuracy: 82.0\n",
      " Loss: 0.38969630869024513. Accuracy: 82.0\n",
      " Loss: 0.38456810749097714. Accuracy: 82.0\n",
      " Loss: 0.3840820384700055. Accuracy: 82.0\n",
      " Loss: 0.39442135645014786. Accuracy: 82.0\n",
      " Loss: 0.3944240557770718. Accuracy: 82.0\n",
      " Loss: 0.41489867526483065. Accuracy: 80.0\n",
      " Loss: 0.4060476829354957. Accuracy: 82.0\n",
      " Loss: 0.41787522379096115. Accuracy: 78.0\n",
      " Loss: 0.39631415346157156. Accuracy: 82.0\n",
      " Loss: 0.39805830710796725. Accuracy: 82.0\n",
      " Loss: 0.39234431553584725. Accuracy: 82.0\n",
      " Loss: 0.39499988856834534. Accuracy: 82.0\n",
      " Loss: 0.38310820579332355. Accuracy: 82.0\n",
      " Loss: 0.38159988430617886. Accuracy: 82.0\n",
      " Loss: 0.38646214928796324. Accuracy: 82.0\n",
      " Loss: 0.4156272419456036. Accuracy: 80.0\n",
      " Loss: 0.3991750247717573. Accuracy: 82.0\n",
      " Loss: 0.3894698148682437. Accuracy: 82.0\n",
      " Loss: 0.3853412410646706. Accuracy: 82.0\n",
      " Loss: 0.38514705007019073. Accuracy: 82.0\n",
      " Loss: 0.38983133046232976. Accuracy: 82.0\n",
      " Loss: 0.3945832426330344. Accuracy: 82.0\n",
      " Loss: 0.3895571217231918. Accuracy: 82.0\n",
      " Loss: 0.40160218604058173. Accuracy: 82.0\n",
      " Loss: 0.3884123332634226. Accuracy: 82.0\n",
      " Loss: 0.38713898297395644. Accuracy: 82.0\n",
      " Loss: 0.38335547066011716. Accuracy: 82.0\n",
      " Loss: 0.3863921627667332. Accuracy: 82.0\n",
      " Loss: 0.3868822555807492. Accuracy: 82.0\n",
      " Loss: 0.391660414829812. Accuracy: 82.0\n",
      " Loss: 0.38578336471716285. Accuracy: 82.0\n",
      " Loss: 0.3878855040740291. Accuracy: 82.0\n",
      " Loss: 0.3874093361294399. Accuracy: 82.0\n",
      " Loss: 0.3885123746727186. Accuracy: 82.0\n",
      " Loss: 0.38620530291464095. Accuracy: 82.0\n",
      " Loss: 0.38460941753880434. Accuracy: 82.0\n",
      " Loss: 0.38749457800489834. Accuracy: 82.0\n",
      " Loss: 0.3827793158437089. Accuracy: 82.0\n",
      " Loss: 0.3873745508228421. Accuracy: 82.0\n",
      " Loss: 0.3881478404682093. Accuracy: 82.0\n",
      " Loss: 0.38909448614499526. Accuracy: 82.0\n",
      " Loss: 0.3862523005100775. Accuracy: 82.0\n",
      " Loss: 0.38168545206727683. Accuracy: 82.0\n",
      " Loss: 0.38303895310473307. Accuracy: 82.0\n",
      " Loss: 0.3875667096964389. Accuracy: 82.0\n",
      " Loss: 0.38065141916134965. Accuracy: 82.0\n",
      " Loss: 0.3964717459986241. Accuracy: 82.0\n",
      " Loss: 0.40285991695423945. Accuracy: 82.0\n",
      " Loss: 0.38841334862987426. Accuracy: 82.0\n",
      " Loss: 0.3858915436569896. Accuracy: 82.0\n",
      " Loss: 0.3825127336830883. Accuracy: 82.0\n",
      " Loss: 0.3806831324298219. Accuracy: 82.0\n",
      " Loss: 0.39118464271671655. Accuracy: 82.0\n",
      " Loss: 0.3879508797096423. Accuracy: 82.0\n",
      " Loss: 0.383777472735992. Accuracy: 82.0\n",
      " Loss: 0.5608523949750088. Accuracy: 68.0\n",
      " Loss: 0.49570904338710536. Accuracy: 74.0\n",
      " Loss: 0.5185372107869626. Accuracy: 74.0\n",
      " Loss: 0.5451920550187788. Accuracy: 66.0\n",
      " Loss: 0.4866463430950171. Accuracy: 74.0\n",
      " Loss: 0.5011590729038289. Accuracy: 74.0\n",
      " Loss: 0.49787107755115356. Accuracy: 74.0\n",
      " Loss: 0.48150971609094995. Accuracy: 74.0\n",
      " Loss: 0.49117762383673835. Accuracy: 74.0\n",
      " Loss: 0.5003585168678001. Accuracy: 74.0\n",
      " Loss: 0.4832830747529078. Accuracy: 74.0\n",
      " Loss: 0.491549617565297. Accuracy: 74.0\n",
      " Loss: 0.4822681825641848. Accuracy: 74.0\n",
      " Loss: 0.48229047561346305. Accuracy: 74.0\n",
      " Loss: 0.4928429376592158. Accuracy: 74.0\n",
      " Loss: 0.4857173337018685. Accuracy: 74.0\n",
      " Loss: 0.49257021535648166. Accuracy: 74.0\n",
      " Loss: 0.4834718040006101. Accuracy: 74.0\n",
      " Loss: 0.4957881165239905. Accuracy: 74.0\n",
      " Loss: 0.4854112129509394. Accuracy: 74.0\n",
      " Loss: 0.48417723485249553. Accuracy: 74.0\n",
      " Loss: 0.48543512202328204. Accuracy: 74.0\n",
      " Loss: 0.48716081353188656. Accuracy: 74.0\n",
      " Loss: 0.5008217577318601. Accuracy: 74.0\n",
      " Loss: 0.48878329416707855. Accuracy: 74.0\n",
      " Loss: 0.479110451341503. Accuracy: 74.0\n",
      " Loss: 0.47815341743538736. Accuracy: 74.0\n",
      " Loss: 0.48347789717891376. Accuracy: 74.0\n",
      " Loss: 0.47801431992813376. Accuracy: 74.0\n",
      " Loss: 0.47940521021208043. Accuracy: 74.0\n",
      " Loss: 0.4835518600245723. Accuracy: 74.0\n",
      " Loss: 0.48306395043310657. Accuracy: 74.0\n",
      " Loss: 0.4876576318547927. Accuracy: 74.0\n",
      " Loss: 0.4818202663876445. Accuracy: 74.0\n",
      " Loss: 0.4857747106278339. Accuracy: 74.0\n",
      " Loss: 0.48375378062064556. Accuracy: 74.0\n",
      " Loss: 0.4877160637467546. Accuracy: 74.0\n",
      " Loss: 0.4950740081510503. Accuracy: 74.0\n",
      " Loss: 0.5038834150748562. Accuracy: 74.0\n",
      " Loss: 0.485151006274632. Accuracy: 74.0\n",
      " Loss: 0.47990305795881794. Accuracy: 74.0\n",
      " Loss: 0.4789616538719247. Accuracy: 74.0\n",
      " Loss: 0.4918975506355946. Accuracy: 74.0\n",
      " Loss: 0.47613074531602023. Accuracy: 74.0\n",
      " Loss: 0.4755896675587951. Accuracy: 74.0\n",
      " Loss: 0.4805254331493529. Accuracy: 74.0\n",
      " Loss: 0.4878189503423823. Accuracy: 74.0\n",
      " Loss: 0.4760196378153159. Accuracy: 74.0\n",
      " Loss: 0.48041070570766353. Accuracy: 74.0\n",
      " Loss: 0.47812163040402994. Accuracy: 74.0\n",
      " Loss: 0.47730463615026564. Accuracy: 74.0\n",
      " Loss: 0.4751014338878122. Accuracy: 74.0\n",
      " Loss: 0.47657744761438153. Accuracy: 74.0\n",
      " Loss: 0.4779278326449821. Accuracy: 74.0\n",
      " Loss: 0.4789312755271203. Accuracy: 74.0\n",
      " Loss: 0.4751345204392942. Accuracy: 74.0\n",
      " Loss: 0.47874290762817734. Accuracy: 74.0\n",
      " Loss: 0.4762660123854744. Accuracy: 74.0\n",
      " Loss: 0.48350365834927744. Accuracy: 74.0\n",
      " Loss: 0.4767981887923338. Accuracy: 74.0\n",
      " Loss: 0.4810524634403123. Accuracy: 74.0\n",
      " Loss: 0.4756650681312635. Accuracy: 74.0\n",
      " Loss: 0.47438869735159644. Accuracy: 74.0\n",
      " Loss: 0.475058498314022. Accuracy: 74.0\n",
      " Loss: 0.4767724583950189. Accuracy: 74.0\n",
      " Loss: 0.47836328045533494. Accuracy: 74.0\n",
      " Loss: 0.475899383643914. Accuracy: 74.0\n",
      " Loss: 0.4796970732951343. Accuracy: 74.0\n",
      " Loss: 0.4776563834195588. Accuracy: 74.0\n",
      " Loss: 0.47525734597143354. Accuracy: 74.0\n",
      " Loss: 0.4798477602943899. Accuracy: 74.0\n",
      " Loss: 0.5552582718080612. Accuracy: 70.0\n",
      " Loss: 0.5113389935525628. Accuracy: 68.0\n",
      " Loss: 0.48483563511033023. Accuracy: 74.0\n",
      " Loss: 0.4827656934362722. Accuracy: 74.0\n",
      " Loss: 0.48461020145674866. Accuracy: 74.0\n",
      " Loss: 0.48103098493939345. Accuracy: 74.0\n",
      " Loss: 0.4773224236612236. Accuracy: 74.0\n",
      " Loss: 0.4774083408637489. Accuracy: 74.0\n",
      " Loss: 0.4840010321282716. Accuracy: 74.0\n",
      " Loss: 0.4807636738670294. Accuracy: 74.0\n",
      " Loss: 0.4763425943127254. Accuracy: 74.0\n",
      " Loss: 0.479264916719103. Accuracy: 74.0\n",
      " Loss: 0.47969376415740955. Accuracy: 74.0\n",
      " Loss: 0.48209010501856936. Accuracy: 74.0\n",
      " Loss: 0.4767853810715246. Accuracy: 74.0\n",
      " Loss: 0.4761800695558418. Accuracy: 74.0\n",
      " Loss: 0.47704714354898214. Accuracy: 74.0\n",
      " Loss: 0.47404343211528155. Accuracy: 74.0\n",
      " Loss: 0.4762329731832097. Accuracy: 74.0\n",
      " Loss: 0.47494594380157307. Accuracy: 74.0\n",
      " Loss: 0.4761914120176482. Accuracy: 74.0\n",
      " Loss: 0.4774961429119833. Accuracy: 74.0\n",
      " Loss: 0.4774679707674625. Accuracy: 74.0\n",
      " Loss: 0.4809165116005147. Accuracy: 74.0\n",
      " Loss: 0.4776885561806523. Accuracy: 74.0\n",
      " Loss: 0.47859150311036275. Accuracy: 74.0\n",
      " Loss: 0.4764304909056193. Accuracy: 74.0\n",
      " Loss: 0.4743523812973558. Accuracy: 74.0\n",
      " Loss: 0.47718869788106416. Accuracy: 74.0\n",
      " Loss: 0.47618136168108094. Accuracy: 74.0\n",
      " Loss: 0.47855903825525503. Accuracy: 74.0\n",
      " Loss: 0.4918177847872687. Accuracy: 74.0\n",
      " Loss: 0.5156516789567468. Accuracy: 72.0\n",
      " Loss: 0.5272894334168086. Accuracy: 74.0\n",
      " Loss: 0.508466143890655. Accuracy: 74.0\n",
      " Loss: 0.4867974400442745. Accuracy: 74.0\n",
      " Loss: 0.4856208657309071. Accuracy: 74.0\n",
      " Loss: 0.4816156192127983. Accuracy: 74.0\n",
      " Loss: 0.47697975547289845. Accuracy: 74.0\n",
      " Loss: 0.48243848161744607. Accuracy: 74.0\n",
      " Loss: 0.47975059177262663. Accuracy: 74.0\n",
      " Loss: 0.4855188153742165. Accuracy: 74.0\n",
      " Loss: 0.48414159126171624. Accuracy: 74.0\n",
      " Loss: 0.49394096772404056. Accuracy: 74.0\n",
      " Loss: 0.4860714610148318. Accuracy: 74.0\n",
      " Loss: 0.4885866345211946. Accuracy: 74.0\n",
      " Loss: 0.4833306194443105. Accuracy: 74.0\n",
      " Loss: 0.48096550987920295. Accuracy: 74.0\n",
      " Loss: 0.48067264234333945. Accuracy: 74.0\n",
      " Loss: 0.485500624078993. Accuracy: 74.0\n",
      " Loss: 0.4812381196760884. Accuracy: 74.0\n",
      " Loss: 0.4796034212132849. Accuracy: 74.0\n",
      " Loss: 0.4780879029318362. Accuracy: 74.0\n",
      " Loss: 0.4836005465281755. Accuracy: 74.0\n",
      " Loss: 0.4745427805296981. Accuracy: 74.0\n",
      " Loss: 0.4723236030918974. Accuracy: 74.0\n",
      " Loss: 0.4762178240764831. Accuracy: 74.0\n",
      " Loss: 0.47590852275174145. Accuracy: 74.0\n",
      " Loss: 0.4729992104977271. Accuracy: 74.0\n",
      " Loss: 0.4805831774139358. Accuracy: 74.0\n",
      " Loss: 0.47502732981116197. Accuracy: 74.0\n",
      " Loss: 0.48215345293832684. Accuracy: 74.0\n",
      " Loss: 0.47845303256795657. Accuracy: 74.0\n",
      " Loss: 0.473671400567398. Accuracy: 74.0\n",
      " Loss: 0.47340058305449817. Accuracy: 74.0\n",
      " Loss: 0.4732828791489726. Accuracy: 74.0\n",
      " Loss: 0.476053667638115. Accuracy: 74.0\n",
      " Loss: 0.4750048249792587. Accuracy: 74.0\n",
      " Loss: 0.47327602230732624. Accuracy: 74.0\n",
      " Loss: 0.4740362281306068. Accuracy: 74.0\n",
      " Loss: 0.4803621658639156. Accuracy: 74.0\n",
      " Loss: 0.4759575613951517. Accuracy: 74.0\n",
      " Loss: 0.47375204194242426. Accuracy: 74.0\n",
      " Loss: 0.4732118552863153. Accuracy: 74.0\n",
      " Loss: 0.4814426426786281. Accuracy: 74.0\n",
      " Loss: 0.4778108928924826. Accuracy: 74.0\n",
      " Loss: 0.4791978449921567. Accuracy: 74.0\n",
      " Loss: 0.48065742767666847. Accuracy: 74.0\n",
      " Loss: 0.47926350289489844. Accuracy: 74.0\n",
      " Loss: 0.473849572719987. Accuracy: 74.0\n",
      " Loss: 0.4770019262885728. Accuracy: 74.0\n",
      " Loss: 0.47408314089650616. Accuracy: 74.0\n",
      " Loss: 0.47384341384720413. Accuracy: 74.0\n",
      " Loss: 0.47604729347589114. Accuracy: 74.0\n",
      " Loss: 0.478374813068765. Accuracy: 74.0\n",
      " Loss: 0.47385983766918455. Accuracy: 74.0\n",
      " Loss: 0.47869548460191935. Accuracy: 74.0\n",
      " Loss: 0.4739327068666148. Accuracy: 74.0\n",
      " Loss: 0.47475022811948064. Accuracy: 74.0\n",
      " Loss: 0.47294512476371664. Accuracy: 74.0\n",
      " Loss: 0.4750007342927688. Accuracy: 74.0\n",
      " Loss: 0.4821865000146477. Accuracy: 74.0\n",
      " Loss: 0.4760434080885875. Accuracy: 74.0\n",
      " Loss: 0.4738830522979674. Accuracy: 74.0\n",
      " Loss: 0.47462173627831133. Accuracy: 74.0\n",
      " Loss: 0.4805236401820707. Accuracy: 74.0\n",
      " Loss: 0.47498204395946003. Accuracy: 74.0\n",
      " Loss: 0.47898004218928464. Accuracy: 74.0\n",
      " Loss: 0.4757709697675796. Accuracy: 74.0\n",
      " Loss: 0.4740206473901321. Accuracy: 74.0\n",
      " Loss: 0.4755423947123177. Accuracy: 74.0\n",
      " Loss: 0.47723375202698437. Accuracy: 74.0\n",
      " Loss: 0.4767653361526004. Accuracy: 74.0\n",
      " Loss: 0.4756425425696193. Accuracy: 74.0\n",
      " Loss: 0.47655159232544747. Accuracy: 74.0\n",
      " Loss: 0.4772152628843446. Accuracy: 74.0\n",
      " Loss: 0.47590887030076373. Accuracy: 74.0\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    for keypoints, labels in dataloader:\n",
    "        # Squeeze unnecessary dimensions\n",
    "        keypoints = keypoints.squeeze(0)  # Now keypoints shape should be [1, 20, 17, 2]\n",
    "        labels = labels.squeeze(0)        # Now labels shape should be [1]\n",
    "        # print('keypoints:',keypoints.shape,'labels:', labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(keypoints)\n",
    "        # print('outputs before',outputs, 'labels before',labels)\n",
    "\n",
    "        outputs = outputs.squeeze()\n",
    "        labels = labels.squeeze().long()\n",
    "        # print('outputs after',outputs, 'labels after',labels)\n",
    "        # break\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        # Total number of labels\n",
    "        total += 1\n",
    "        \n",
    "        # Total correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy =  correct / total * 100\n",
    "    epoch_loss = running_loss / len(dataloader)        \n",
    "            # Print Loss\n",
    "    print(' Loss: {}. Accuracy: {}'.format( epoch_loss, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Total correct predictions\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Calculate accuracy and loss for the epoch\u001b[39;00m\n\u001b[1;32m     40\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m total \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (32) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard\n",
    "writer = SummaryWriter(log_dir='./runs/convLSTM')\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for keypoints, labels in dataloader:\n",
    "        # Squeeze unnecessary dimensions\n",
    "        keypoints = keypoints.squeeze(0)  # Now keypoints shape should be [1, 20, 17, 2]\n",
    "        labels = labels.squeeze(0)        # Now labels shape should be [1]\n",
    "        # print('keypoints:',keypoints.shape,'labels:', labels.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(keypoints)\n",
    "        # print('outputs before',outputs, 'labels before',labels)\n",
    "\n",
    "        outputs = outputs.squeeze()\n",
    "        labels = labels.squeeze().long()\n",
    "        # print('outputs after',outputs, 'labels after',labels)\n",
    "        # break\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 0)\n",
    "        # Total number of labels\n",
    "        total += 1\n",
    "        \n",
    "        # Total correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate accuracy and loss for the epoch\n",
    "    accuracy = correct / total * 100\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Log loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvLSTMModel(nn.Module):\n",
    "    def __init__(self, input_shape, hidden_dim, num_classes):\n",
    "        super(ConvLSTMModel, self).__init__()\n",
    "        self.segment_length, self.keypoints, self.coords = input_shape\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1d_1 = nn.Conv1d(in_channels=self.keypoints * self.coords, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conv1d_2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "\n",
    "        # Adaptive pooling\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool1d(output_size=8)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Moderate dropout rate\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm_input_size = 32  # Output channels from last Conv1d layer\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_size, hidden_size=hidden_dim, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input\n",
    "        x = x.view(x.size(0), self.keypoints * self.coords, self.segment_length)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = F.relu(self.bn1(self.conv1d_1(x)))\n",
    "        x = F.relu(self.bn2(self.conv1d_2(x)))\n",
    "\n",
    "        # Apply adaptive pooling to ensure output size is appropriate\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        # Flatten the features\n",
    "        x = x.transpose(1, 2)  # (batch_size, seq_len, num_features)\n",
    "\n",
    "        # LSTM layer\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        # Use only the output of the last time step\n",
    "        x = x[:, -1, :]\n",
    "\n",
    "        # Apply dropout manually\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume 'model' is an instance of your model class\n",
    "model = ConvLSTMModel(input_shape=(7, 33, 2), hidden_dim=64, num_classes=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for evaluating the model on the validation set\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for keypoints, labels in val_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(keypoints)  # outputs shape should be [batch_size, 1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_labels = outputs.round()  # Round the outputs to get the final predictions for binary classification\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Train Loss: 0.6417, Train Accuracy: 61.43%\n",
      "Epoch 1/500 - Validation Loss: 0.5752, Validation Accuracy: 0.7206\n",
      "Best model saved at epoch 1 with val acc: 0.7206\n",
      "Epoch [2/500], Train Loss: 0.5104, Train Accuracy: 73.59%\n",
      "Epoch 2/500 - Validation Loss: 0.4633, Validation Accuracy: 0.7812\n",
      "Best model saved at epoch 2 with val acc: 0.7812\n",
      "Epoch [3/500], Train Loss: 0.4754, Train Accuracy: 75.58%\n",
      "Epoch 3/500 - Validation Loss: 0.4745, Validation Accuracy: 0.7684\n",
      "Epoch [4/500], Train Loss: 0.4580, Train Accuracy: 77.70%\n",
      "Epoch 4/500 - Validation Loss: 0.4668, Validation Accuracy: 0.7757\n",
      "Epoch [5/500], Train Loss: 0.4534, Train Accuracy: 76.73%\n",
      "Epoch 5/500 - Validation Loss: 0.4714, Validation Accuracy: 0.7684\n",
      "Epoch [6/500], Train Loss: 0.4403, Train Accuracy: 78.11%\n",
      "Epoch 6/500 - Validation Loss: 0.4228, Validation Accuracy: 0.7978\n",
      "Best model saved at epoch 6 with val acc: 0.7978\n",
      "Epoch [7/500], Train Loss: 0.4329, Train Accuracy: 78.48%\n",
      "Epoch 7/500 - Validation Loss: 0.4008, Validation Accuracy: 0.8180\n",
      "Best model saved at epoch 7 with val acc: 0.8180\n",
      "Epoch [8/500], Train Loss: 0.4235, Train Accuracy: 79.40%\n",
      "Epoch 8/500 - Validation Loss: 0.4108, Validation Accuracy: 0.8162\n",
      "Epoch [9/500], Train Loss: 0.4231, Train Accuracy: 78.94%\n",
      "Epoch 9/500 - Validation Loss: 0.4325, Validation Accuracy: 0.7996\n",
      "Epoch [10/500], Train Loss: 0.4295, Train Accuracy: 78.53%\n",
      "Epoch 10/500 - Validation Loss: 0.4053, Validation Accuracy: 0.7978\n",
      "Epoch [11/500], Train Loss: 0.4252, Train Accuracy: 79.26%\n",
      "Epoch 11/500 - Validation Loss: 0.5073, Validation Accuracy: 0.7482\n",
      "Epoch [12/500], Train Loss: 0.4216, Train Accuracy: 78.66%\n",
      "Epoch 12/500 - Validation Loss: 0.4847, Validation Accuracy: 0.7537\n",
      "Epoch [13/500], Train Loss: 0.4050, Train Accuracy: 79.59%\n",
      "Epoch 13/500 - Validation Loss: 0.4059, Validation Accuracy: 0.8199\n",
      "Best model saved at epoch 13 with val acc: 0.8199\n",
      "Epoch [14/500], Train Loss: 0.4107, Train Accuracy: 79.59%\n",
      "Epoch 14/500 - Validation Loss: 0.4267, Validation Accuracy: 0.7757\n",
      "Epoch [15/500], Train Loss: 0.4118, Train Accuracy: 79.68%\n",
      "Epoch 15/500 - Validation Loss: 0.4255, Validation Accuracy: 0.8070\n",
      "Epoch [16/500], Train Loss: 0.3967, Train Accuracy: 79.82%\n",
      "Epoch 16/500 - Validation Loss: 0.4072, Validation Accuracy: 0.8033\n",
      "Epoch [17/500], Train Loss: 0.4003, Train Accuracy: 79.49%\n",
      "Epoch 17/500 - Validation Loss: 0.3894, Validation Accuracy: 0.8199\n",
      "Epoch [18/500], Train Loss: 0.3953, Train Accuracy: 79.91%\n",
      "Epoch 18/500 - Validation Loss: 0.4267, Validation Accuracy: 0.7923\n",
      "Epoch [19/500], Train Loss: 0.3871, Train Accuracy: 80.46%\n",
      "Epoch 19/500 - Validation Loss: 0.4006, Validation Accuracy: 0.7996\n",
      "Epoch [20/500], Train Loss: 0.3881, Train Accuracy: 80.60%\n",
      "Epoch 20/500 - Validation Loss: 0.4108, Validation Accuracy: 0.7978\n",
      "Epoch [21/500], Train Loss: 0.3889, Train Accuracy: 79.72%\n",
      "Epoch 21/500 - Validation Loss: 0.3995, Validation Accuracy: 0.8107\n",
      "Epoch [22/500], Train Loss: 0.3930, Train Accuracy: 80.32%\n",
      "Epoch 22/500 - Validation Loss: 0.4408, Validation Accuracy: 0.7831\n",
      "Epoch [23/500], Train Loss: 0.3775, Train Accuracy: 80.51%\n",
      "Epoch 23/500 - Validation Loss: 0.3997, Validation Accuracy: 0.8107\n",
      "Epoch [24/500], Train Loss: 0.3735, Train Accuracy: 81.38%\n",
      "Epoch 24/500 - Validation Loss: 0.3802, Validation Accuracy: 0.8254\n",
      "Best model saved at epoch 24 with val acc: 0.8254\n",
      "Epoch [25/500], Train Loss: 0.3828, Train Accuracy: 79.86%\n",
      "Epoch 25/500 - Validation Loss: 0.4178, Validation Accuracy: 0.8033\n",
      "Epoch [26/500], Train Loss: 0.3810, Train Accuracy: 80.23%\n",
      "Epoch 26/500 - Validation Loss: 0.3709, Validation Accuracy: 0.8162\n",
      "Epoch [27/500], Train Loss: 0.3729, Train Accuracy: 81.20%\n",
      "Epoch 27/500 - Validation Loss: 0.4152, Validation Accuracy: 0.8015\n",
      "Epoch [28/500], Train Loss: 0.3747, Train Accuracy: 80.46%\n",
      "Epoch 28/500 - Validation Loss: 0.3803, Validation Accuracy: 0.8125\n",
      "Epoch [29/500], Train Loss: 0.3676, Train Accuracy: 80.92%\n",
      "Epoch 29/500 - Validation Loss: 0.4158, Validation Accuracy: 0.8015\n",
      "Epoch [30/500], Train Loss: 0.3628, Train Accuracy: 81.57%\n",
      "Epoch 30/500 - Validation Loss: 0.3776, Validation Accuracy: 0.8254\n",
      "Epoch [31/500], Train Loss: 0.3653, Train Accuracy: 80.41%\n",
      "Epoch 31/500 - Validation Loss: 0.3696, Validation Accuracy: 0.8199\n",
      "Epoch [32/500], Train Loss: 0.3544, Train Accuracy: 82.07%\n",
      "Epoch 32/500 - Validation Loss: 0.3459, Validation Accuracy: 0.8456\n",
      "Best model saved at epoch 32 with val acc: 0.8456\n",
      "Epoch [33/500], Train Loss: 0.3545, Train Accuracy: 82.03%\n",
      "Epoch 33/500 - Validation Loss: 0.3520, Validation Accuracy: 0.8346\n",
      "Epoch [34/500], Train Loss: 0.3409, Train Accuracy: 82.21%\n",
      "Epoch 34/500 - Validation Loss: 0.3972, Validation Accuracy: 0.8033\n",
      "Epoch [35/500], Train Loss: 0.3540, Train Accuracy: 81.80%\n",
      "Epoch 35/500 - Validation Loss: 0.3557, Validation Accuracy: 0.8364\n",
      "Epoch [36/500], Train Loss: 0.3405, Train Accuracy: 83.27%\n",
      "Epoch 36/500 - Validation Loss: 0.3454, Validation Accuracy: 0.8290\n",
      "Epoch [37/500], Train Loss: 0.3371, Train Accuracy: 83.23%\n",
      "Epoch 37/500 - Validation Loss: 0.3541, Validation Accuracy: 0.8382\n",
      "Epoch [38/500], Train Loss: 0.3371, Train Accuracy: 83.32%\n",
      "Epoch 38/500 - Validation Loss: 0.3507, Validation Accuracy: 0.8401\n",
      "Epoch [39/500], Train Loss: 0.3293, Train Accuracy: 84.56%\n",
      "Epoch 39/500 - Validation Loss: 0.3608, Validation Accuracy: 0.8107\n",
      "Epoch [40/500], Train Loss: 0.3301, Train Accuracy: 84.10%\n",
      "Epoch 40/500 - Validation Loss: 0.3501, Validation Accuracy: 0.8290\n",
      "Epoch [41/500], Train Loss: 0.3302, Train Accuracy: 83.96%\n",
      "Epoch 41/500 - Validation Loss: 0.3646, Validation Accuracy: 0.8217\n",
      "Epoch [42/500], Train Loss: 0.3078, Train Accuracy: 85.76%\n",
      "Epoch 42/500 - Validation Loss: 0.3539, Validation Accuracy: 0.8493\n",
      "Best model saved at epoch 42 with val acc: 0.8493\n",
      "Epoch [43/500], Train Loss: 0.3223, Train Accuracy: 84.24%\n",
      "Epoch 43/500 - Validation Loss: 0.3408, Validation Accuracy: 0.8364\n",
      "Epoch [44/500], Train Loss: 0.3064, Train Accuracy: 85.48%\n",
      "Epoch 44/500 - Validation Loss: 0.3540, Validation Accuracy: 0.8235\n",
      "Epoch [45/500], Train Loss: 0.3054, Train Accuracy: 86.13%\n",
      "Epoch 45/500 - Validation Loss: 0.3531, Validation Accuracy: 0.8235\n",
      "Epoch [46/500], Train Loss: 0.3136, Train Accuracy: 85.12%\n",
      "Epoch 46/500 - Validation Loss: 0.3378, Validation Accuracy: 0.8309\n",
      "Epoch [47/500], Train Loss: 0.3001, Train Accuracy: 86.54%\n",
      "Epoch 47/500 - Validation Loss: 0.3333, Validation Accuracy: 0.8364\n",
      "Epoch [48/500], Train Loss: 0.2971, Train Accuracy: 85.90%\n",
      "Epoch 48/500 - Validation Loss: 0.3076, Validation Accuracy: 0.8603\n",
      "Best model saved at epoch 48 with val acc: 0.8603\n",
      "Epoch [49/500], Train Loss: 0.2924, Train Accuracy: 86.45%\n",
      "Epoch 49/500 - Validation Loss: 0.3016, Validation Accuracy: 0.8695\n",
      "Best model saved at epoch 49 with val acc: 0.8695\n",
      "Epoch [50/500], Train Loss: 0.2926, Train Accuracy: 86.59%\n",
      "Epoch 50/500 - Validation Loss: 0.3056, Validation Accuracy: 0.8695\n",
      "Epoch [51/500], Train Loss: 0.2880, Train Accuracy: 86.59%\n",
      "Epoch 51/500 - Validation Loss: 0.3058, Validation Accuracy: 0.8732\n",
      "Best model saved at epoch 51 with val acc: 0.8732\n",
      "Epoch [52/500], Train Loss: 0.2884, Train Accuracy: 86.27%\n",
      "Epoch 52/500 - Validation Loss: 0.3058, Validation Accuracy: 0.8585\n",
      "Epoch [53/500], Train Loss: 0.2815, Train Accuracy: 87.10%\n",
      "Epoch 53/500 - Validation Loss: 0.3008, Validation Accuracy: 0.8603\n",
      "Epoch [54/500], Train Loss: 0.2803, Train Accuracy: 87.33%\n",
      "Epoch 54/500 - Validation Loss: 0.2930, Validation Accuracy: 0.8676\n",
      "Epoch [55/500], Train Loss: 0.2871, Train Accuracy: 86.59%\n",
      "Epoch 55/500 - Validation Loss: 0.3226, Validation Accuracy: 0.8364\n",
      "Epoch [56/500], Train Loss: 0.2811, Train Accuracy: 87.28%\n",
      "Epoch 56/500 - Validation Loss: 0.3218, Validation Accuracy: 0.8493\n",
      "Epoch [57/500], Train Loss: 0.2762, Train Accuracy: 87.56%\n",
      "Epoch 57/500 - Validation Loss: 0.3380, Validation Accuracy: 0.8474\n",
      "Epoch [58/500], Train Loss: 0.2709, Train Accuracy: 88.39%\n",
      "Epoch 58/500 - Validation Loss: 0.3095, Validation Accuracy: 0.8658\n",
      "Epoch [59/500], Train Loss: 0.2628, Train Accuracy: 88.25%\n",
      "Epoch 59/500 - Validation Loss: 0.2872, Validation Accuracy: 0.8768\n",
      "Best model saved at epoch 59 with val acc: 0.8768\n",
      "Epoch [60/500], Train Loss: 0.2637, Train Accuracy: 87.83%\n",
      "Epoch 60/500 - Validation Loss: 0.3223, Validation Accuracy: 0.8438\n",
      "Epoch [61/500], Train Loss: 0.2553, Train Accuracy: 88.71%\n",
      "Epoch 61/500 - Validation Loss: 0.2962, Validation Accuracy: 0.8548\n",
      "Epoch [62/500], Train Loss: 0.2655, Train Accuracy: 87.93%\n",
      "Epoch 62/500 - Validation Loss: 0.2879, Validation Accuracy: 0.8676\n",
      "Epoch [63/500], Train Loss: 0.2574, Train Accuracy: 88.89%\n",
      "Epoch 63/500 - Validation Loss: 0.2815, Validation Accuracy: 0.8952\n",
      "Best model saved at epoch 63 with val acc: 0.8952\n",
      "Epoch [64/500], Train Loss: 0.2475, Train Accuracy: 88.53%\n",
      "Epoch 64/500 - Validation Loss: 0.2742, Validation Accuracy: 0.8824\n",
      "Epoch [65/500], Train Loss: 0.2466, Train Accuracy: 89.31%\n",
      "Epoch 65/500 - Validation Loss: 0.3336, Validation Accuracy: 0.8511\n",
      "Epoch [66/500], Train Loss: 0.2528, Train Accuracy: 88.20%\n",
      "Epoch 66/500 - Validation Loss: 0.2779, Validation Accuracy: 0.8824\n",
      "Epoch [67/500], Train Loss: 0.2515, Train Accuracy: 88.89%\n",
      "Epoch 67/500 - Validation Loss: 0.2889, Validation Accuracy: 0.8658\n",
      "Epoch [68/500], Train Loss: 0.2444, Train Accuracy: 89.63%\n",
      "Epoch 68/500 - Validation Loss: 0.2668, Validation Accuracy: 0.8971\n",
      "Best model saved at epoch 68 with val acc: 0.8971\n",
      "Epoch [69/500], Train Loss: 0.2360, Train Accuracy: 89.49%\n",
      "Epoch 69/500 - Validation Loss: 0.2936, Validation Accuracy: 0.8713\n",
      "Epoch [70/500], Train Loss: 0.2426, Train Accuracy: 89.03%\n",
      "Epoch 70/500 - Validation Loss: 0.2890, Validation Accuracy: 0.8676\n",
      "Epoch [71/500], Train Loss: 0.2449, Train Accuracy: 88.48%\n",
      "Epoch 71/500 - Validation Loss: 0.3293, Validation Accuracy: 0.8493\n",
      "Epoch [72/500], Train Loss: 0.2370, Train Accuracy: 89.72%\n",
      "Epoch 72/500 - Validation Loss: 0.3154, Validation Accuracy: 0.8566\n",
      "Epoch [73/500], Train Loss: 0.2239, Train Accuracy: 90.00%\n",
      "Epoch 73/500 - Validation Loss: 0.3096, Validation Accuracy: 0.8713\n",
      "Epoch [74/500], Train Loss: 0.2309, Train Accuracy: 90.32%\n",
      "Epoch 74/500 - Validation Loss: 0.2760, Validation Accuracy: 0.8934\n",
      "Epoch [75/500], Train Loss: 0.2301, Train Accuracy: 89.08%\n",
      "Epoch 75/500 - Validation Loss: 0.2909, Validation Accuracy: 0.8805\n",
      "Epoch [76/500], Train Loss: 0.2260, Train Accuracy: 89.54%\n",
      "Epoch 76/500 - Validation Loss: 0.2624, Validation Accuracy: 0.8915\n",
      "Epoch [77/500], Train Loss: 0.2207, Train Accuracy: 90.51%\n",
      "Epoch 77/500 - Validation Loss: 0.2855, Validation Accuracy: 0.8695\n",
      "Epoch [78/500], Train Loss: 0.2378, Train Accuracy: 89.59%\n",
      "Epoch 78/500 - Validation Loss: 0.3207, Validation Accuracy: 0.8585\n",
      "Epoch [79/500], Train Loss: 0.2356, Train Accuracy: 89.22%\n",
      "Epoch 79/500 - Validation Loss: 0.2602, Validation Accuracy: 0.8750\n",
      "Epoch [80/500], Train Loss: 0.2090, Train Accuracy: 91.01%\n",
      "Epoch 80/500 - Validation Loss: 0.2725, Validation Accuracy: 0.8676\n",
      "Epoch [81/500], Train Loss: 0.2167, Train Accuracy: 90.32%\n",
      "Epoch 81/500 - Validation Loss: 0.2783, Validation Accuracy: 0.8732\n",
      "Epoch [82/500], Train Loss: 0.2129, Train Accuracy: 90.41%\n",
      "Epoch 82/500 - Validation Loss: 0.2666, Validation Accuracy: 0.8805\n",
      "Epoch [83/500], Train Loss: 0.2075, Train Accuracy: 91.61%\n",
      "Epoch 83/500 - Validation Loss: 0.3166, Validation Accuracy: 0.8621\n",
      "Epoch [84/500], Train Loss: 0.2038, Train Accuracy: 91.34%\n",
      "Epoch 84/500 - Validation Loss: 0.2541, Validation Accuracy: 0.8805\n",
      "Epoch [85/500], Train Loss: 0.2069, Train Accuracy: 91.75%\n",
      "Epoch 85/500 - Validation Loss: 0.2712, Validation Accuracy: 0.8934\n",
      "Epoch [86/500], Train Loss: 0.2074, Train Accuracy: 91.34%\n",
      "Epoch 86/500 - Validation Loss: 0.2607, Validation Accuracy: 0.8860\n",
      "Epoch [87/500], Train Loss: 0.2024, Train Accuracy: 91.66%\n",
      "Epoch 87/500 - Validation Loss: 0.2731, Validation Accuracy: 0.8732\n",
      "Epoch [88/500], Train Loss: 0.2027, Train Accuracy: 90.69%\n",
      "Epoch 88/500 - Validation Loss: 0.2785, Validation Accuracy: 0.8640\n",
      "Epoch [89/500], Train Loss: 0.2044, Train Accuracy: 91.29%\n",
      "Epoch 89/500 - Validation Loss: 0.2686, Validation Accuracy: 0.8824\n",
      "Epoch [90/500], Train Loss: 0.2074, Train Accuracy: 91.24%\n",
      "Epoch 90/500 - Validation Loss: 0.2723, Validation Accuracy: 0.8713\n",
      "Epoch [91/500], Train Loss: 0.1879, Train Accuracy: 91.71%\n",
      "Epoch 91/500 - Validation Loss: 0.2895, Validation Accuracy: 0.8915\n",
      "Epoch [92/500], Train Loss: 0.1976, Train Accuracy: 91.01%\n",
      "Epoch 92/500 - Validation Loss: 0.2587, Validation Accuracy: 0.8879\n",
      "Epoch [93/500], Train Loss: 0.1836, Train Accuracy: 92.21%\n",
      "Epoch 93/500 - Validation Loss: 0.2474, Validation Accuracy: 0.9081\n",
      "Best model saved at epoch 93 with val acc: 0.9081\n",
      "Epoch [94/500], Train Loss: 0.1928, Train Accuracy: 92.30%\n",
      "Epoch 94/500 - Validation Loss: 0.3047, Validation Accuracy: 0.8695\n",
      "Epoch [95/500], Train Loss: 0.1888, Train Accuracy: 92.58%\n",
      "Epoch 95/500 - Validation Loss: 0.2970, Validation Accuracy: 0.8621\n",
      "Epoch [96/500], Train Loss: 0.1828, Train Accuracy: 92.07%\n",
      "Epoch 96/500 - Validation Loss: 0.2718, Validation Accuracy: 0.8805\n",
      "Epoch [97/500], Train Loss: 0.1872, Train Accuracy: 92.07%\n",
      "Epoch 97/500 - Validation Loss: 0.3003, Validation Accuracy: 0.8750\n",
      "Epoch [98/500], Train Loss: 0.1854, Train Accuracy: 91.75%\n",
      "Epoch 98/500 - Validation Loss: 0.3310, Validation Accuracy: 0.8658\n",
      "Epoch [99/500], Train Loss: 0.1874, Train Accuracy: 91.98%\n",
      "Epoch 99/500 - Validation Loss: 0.3062, Validation Accuracy: 0.8585\n",
      "Epoch [100/500], Train Loss: 0.1845, Train Accuracy: 92.40%\n",
      "Epoch 100/500 - Validation Loss: 0.2641, Validation Accuracy: 0.8824\n",
      "Epoch [101/500], Train Loss: 0.1755, Train Accuracy: 92.81%\n",
      "Epoch 101/500 - Validation Loss: 0.2705, Validation Accuracy: 0.8787\n",
      "Epoch [102/500], Train Loss: 0.1772, Train Accuracy: 92.53%\n",
      "Epoch 102/500 - Validation Loss: 0.2844, Validation Accuracy: 0.8842\n",
      "Epoch [103/500], Train Loss: 0.1860, Train Accuracy: 92.12%\n",
      "Epoch 103/500 - Validation Loss: 0.2517, Validation Accuracy: 0.9062\n",
      "Epoch [104/500], Train Loss: 0.1768, Train Accuracy: 92.58%\n",
      "Epoch 104/500 - Validation Loss: 0.2934, Validation Accuracy: 0.8621\n",
      "Epoch [105/500], Train Loss: 0.1761, Train Accuracy: 92.44%\n",
      "Epoch 105/500 - Validation Loss: 0.2621, Validation Accuracy: 0.8824\n",
      "Epoch [106/500], Train Loss: 0.1757, Train Accuracy: 92.44%\n",
      "Epoch 106/500 - Validation Loss: 0.2700, Validation Accuracy: 0.8915\n",
      "Epoch [107/500], Train Loss: 0.1583, Train Accuracy: 93.09%\n",
      "Epoch 107/500 - Validation Loss: 0.2763, Validation Accuracy: 0.8915\n",
      "Epoch [108/500], Train Loss: 0.1758, Train Accuracy: 92.53%\n",
      "Epoch 108/500 - Validation Loss: 0.2962, Validation Accuracy: 0.8732\n",
      "Epoch [109/500], Train Loss: 0.1605, Train Accuracy: 93.23%\n",
      "Epoch 109/500 - Validation Loss: 0.2992, Validation Accuracy: 0.8824\n",
      "Epoch [110/500], Train Loss: 0.1605, Train Accuracy: 93.13%\n",
      "Epoch 110/500 - Validation Loss: 0.2479, Validation Accuracy: 0.8915\n",
      "Epoch [111/500], Train Loss: 0.1668, Train Accuracy: 92.67%\n",
      "Epoch 111/500 - Validation Loss: 0.2599, Validation Accuracy: 0.8971\n",
      "Epoch [112/500], Train Loss: 0.1574, Train Accuracy: 93.23%\n",
      "Epoch 112/500 - Validation Loss: 0.2437, Validation Accuracy: 0.9118\n",
      "Best model saved at epoch 112 with val acc: 0.9118\n",
      "Epoch [113/500], Train Loss: 0.1622, Train Accuracy: 92.86%\n",
      "Epoch 113/500 - Validation Loss: 0.2553, Validation Accuracy: 0.8897\n",
      "Epoch [114/500], Train Loss: 0.1527, Train Accuracy: 93.23%\n",
      "Epoch 114/500 - Validation Loss: 0.3136, Validation Accuracy: 0.8695\n",
      "Epoch [115/500], Train Loss: 0.1493, Train Accuracy: 93.69%\n",
      "Epoch 115/500 - Validation Loss: 0.3185, Validation Accuracy: 0.8768\n",
      "Epoch [116/500], Train Loss: 0.1517, Train Accuracy: 93.36%\n",
      "Epoch 116/500 - Validation Loss: 0.2381, Validation Accuracy: 0.9099\n",
      "Epoch [117/500], Train Loss: 0.1470, Train Accuracy: 93.36%\n",
      "Epoch 117/500 - Validation Loss: 0.2975, Validation Accuracy: 0.8860\n",
      "Epoch [118/500], Train Loss: 0.1647, Train Accuracy: 92.72%\n",
      "Epoch 118/500 - Validation Loss: 0.2767, Validation Accuracy: 0.8860\n",
      "Epoch [119/500], Train Loss: 0.1468, Train Accuracy: 94.15%\n",
      "Epoch 119/500 - Validation Loss: 0.2893, Validation Accuracy: 0.8787\n",
      "Epoch [120/500], Train Loss: 0.1402, Train Accuracy: 94.19%\n",
      "Epoch 120/500 - Validation Loss: 0.2778, Validation Accuracy: 0.8805\n",
      "Epoch [121/500], Train Loss: 0.1714, Train Accuracy: 92.58%\n",
      "Epoch 121/500 - Validation Loss: 0.2931, Validation Accuracy: 0.8879\n",
      "Epoch [122/500], Train Loss: 0.1523, Train Accuracy: 93.36%\n",
      "Epoch 122/500 - Validation Loss: 0.3752, Validation Accuracy: 0.8695\n",
      "Epoch [123/500], Train Loss: 0.1488, Train Accuracy: 93.36%\n",
      "Epoch 123/500 - Validation Loss: 0.2818, Validation Accuracy: 0.8860\n",
      "Epoch [124/500], Train Loss: 0.1426, Train Accuracy: 94.01%\n",
      "Epoch 124/500 - Validation Loss: 0.3040, Validation Accuracy: 0.8860\n",
      "Epoch [125/500], Train Loss: 0.1587, Train Accuracy: 93.87%\n",
      "Epoch 125/500 - Validation Loss: 0.3251, Validation Accuracy: 0.8824\n",
      "Epoch [126/500], Train Loss: 0.1374, Train Accuracy: 94.33%\n",
      "Epoch 126/500 - Validation Loss: 0.2525, Validation Accuracy: 0.8952\n",
      "Epoch [127/500], Train Loss: 0.1288, Train Accuracy: 94.47%\n",
      "Epoch 127/500 - Validation Loss: 0.2964, Validation Accuracy: 0.9081\n",
      "Epoch [128/500], Train Loss: 0.1448, Train Accuracy: 94.19%\n",
      "Epoch 128/500 - Validation Loss: 0.2964, Validation Accuracy: 0.8676\n",
      "Epoch [129/500], Train Loss: 0.1365, Train Accuracy: 94.42%\n",
      "Epoch 129/500 - Validation Loss: 0.3444, Validation Accuracy: 0.8658\n",
      "Epoch [130/500], Train Loss: 0.1334, Train Accuracy: 94.29%\n",
      "Epoch 130/500 - Validation Loss: 0.2890, Validation Accuracy: 0.8879\n",
      "Epoch [131/500], Train Loss: 0.1309, Train Accuracy: 94.24%\n",
      "Epoch 131/500 - Validation Loss: 0.3089, Validation Accuracy: 0.8842\n",
      "Epoch [132/500], Train Loss: 0.1357, Train Accuracy: 94.42%\n",
      "Epoch 132/500 - Validation Loss: 0.3154, Validation Accuracy: 0.8860\n",
      "Epoch [133/500], Train Loss: 0.1280, Train Accuracy: 94.84%\n",
      "Epoch 133/500 - Validation Loss: 0.2467, Validation Accuracy: 0.9007\n",
      "Epoch [134/500], Train Loss: 0.1311, Train Accuracy: 94.70%\n",
      "Epoch 134/500 - Validation Loss: 0.3122, Validation Accuracy: 0.8805\n",
      "Epoch [135/500], Train Loss: 0.1284, Train Accuracy: 94.93%\n",
      "Epoch 135/500 - Validation Loss: 0.2638, Validation Accuracy: 0.9026\n",
      "Epoch [136/500], Train Loss: 0.1257, Train Accuracy: 94.84%\n",
      "Epoch 136/500 - Validation Loss: 0.2505, Validation Accuracy: 0.9118\n",
      "Epoch [137/500], Train Loss: 0.1269, Train Accuracy: 94.79%\n",
      "Epoch 137/500 - Validation Loss: 0.3386, Validation Accuracy: 0.8695\n",
      "Epoch [138/500], Train Loss: 0.1275, Train Accuracy: 94.56%\n",
      "Epoch 138/500 - Validation Loss: 0.2479, Validation Accuracy: 0.8989\n",
      "Epoch [139/500], Train Loss: 0.1130, Train Accuracy: 95.21%\n",
      "Epoch 139/500 - Validation Loss: 0.3001, Validation Accuracy: 0.8934\n",
      "Epoch [140/500], Train Loss: 0.1237, Train Accuracy: 94.61%\n",
      "Epoch 140/500 - Validation Loss: 0.2862, Validation Accuracy: 0.8897\n",
      "Epoch [141/500], Train Loss: 0.1390, Train Accuracy: 94.56%\n",
      "Epoch 141/500 - Validation Loss: 0.3021, Validation Accuracy: 0.8732\n",
      "Epoch [142/500], Train Loss: 0.1342, Train Accuracy: 94.15%\n",
      "Epoch 142/500 - Validation Loss: 0.2625, Validation Accuracy: 0.8989\n",
      "Epoch [143/500], Train Loss: 0.1247, Train Accuracy: 94.61%\n",
      "Epoch 143/500 - Validation Loss: 0.2759, Validation Accuracy: 0.8879\n",
      "Epoch [144/500], Train Loss: 0.1108, Train Accuracy: 95.25%\n",
      "Epoch 144/500 - Validation Loss: 0.2969, Validation Accuracy: 0.8805\n",
      "Epoch [145/500], Train Loss: 0.1293, Train Accuracy: 94.33%\n",
      "Epoch 145/500 - Validation Loss: 0.2479, Validation Accuracy: 0.9062\n",
      "Epoch [146/500], Train Loss: 0.1188, Train Accuracy: 94.42%\n",
      "Epoch 146/500 - Validation Loss: 0.3089, Validation Accuracy: 0.8897\n",
      "Epoch [147/500], Train Loss: 0.1201, Train Accuracy: 95.07%\n",
      "Epoch 147/500 - Validation Loss: 0.2922, Validation Accuracy: 0.8824\n",
      "Epoch [148/500], Train Loss: 0.1147, Train Accuracy: 95.53%\n",
      "Epoch 148/500 - Validation Loss: 0.2684, Validation Accuracy: 0.9044\n",
      "Epoch [149/500], Train Loss: 0.1234, Train Accuracy: 94.93%\n",
      "Epoch 149/500 - Validation Loss: 0.3133, Validation Accuracy: 0.8842\n",
      "Epoch [150/500], Train Loss: 0.1284, Train Accuracy: 94.56%\n",
      "Epoch 150/500 - Validation Loss: 0.3028, Validation Accuracy: 0.8897\n",
      "Epoch [151/500], Train Loss: 0.1121, Train Accuracy: 95.25%\n",
      "Epoch 151/500 - Validation Loss: 0.2610, Validation Accuracy: 0.8971\n",
      "Epoch [152/500], Train Loss: 0.1227, Train Accuracy: 95.02%\n",
      "Epoch 152/500 - Validation Loss: 0.3229, Validation Accuracy: 0.8750\n",
      "Epoch [153/500], Train Loss: 0.1134, Train Accuracy: 94.93%\n",
      "Epoch 153/500 - Validation Loss: 0.3990, Validation Accuracy: 0.8658\n",
      "Epoch [154/500], Train Loss: 0.1040, Train Accuracy: 95.90%\n",
      "Epoch 154/500 - Validation Loss: 0.3236, Validation Accuracy: 0.8915\n",
      "Epoch [155/500], Train Loss: 0.1183, Train Accuracy: 94.98%\n",
      "Epoch 155/500 - Validation Loss: 0.2477, Validation Accuracy: 0.9099\n",
      "Epoch [156/500], Train Loss: 0.1186, Train Accuracy: 94.93%\n",
      "Epoch 156/500 - Validation Loss: 0.3068, Validation Accuracy: 0.8952\n",
      "Epoch [157/500], Train Loss: 0.1075, Train Accuracy: 95.62%\n",
      "Epoch 157/500 - Validation Loss: 0.2794, Validation Accuracy: 0.8860\n",
      "Epoch [158/500], Train Loss: 0.1072, Train Accuracy: 95.39%\n",
      "Epoch 158/500 - Validation Loss: 0.2812, Validation Accuracy: 0.8952\n",
      "Epoch [159/500], Train Loss: 0.1014, Train Accuracy: 95.48%\n",
      "Epoch 159/500 - Validation Loss: 0.2460, Validation Accuracy: 0.9044\n",
      "Epoch [160/500], Train Loss: 0.1005, Train Accuracy: 95.99%\n",
      "Epoch 160/500 - Validation Loss: 0.3146, Validation Accuracy: 0.8934\n",
      "Epoch [161/500], Train Loss: 0.0951, Train Accuracy: 96.08%\n",
      "Epoch 161/500 - Validation Loss: 0.4208, Validation Accuracy: 0.8456\n",
      "Epoch [162/500], Train Loss: 0.1153, Train Accuracy: 95.02%\n",
      "Epoch 162/500 - Validation Loss: 0.2739, Validation Accuracy: 0.9062\n",
      "Epoch [163/500], Train Loss: 0.0961, Train Accuracy: 95.90%\n",
      "Epoch 163/500 - Validation Loss: 0.2724, Validation Accuracy: 0.8915\n",
      "Epoch [164/500], Train Loss: 0.1237, Train Accuracy: 95.16%\n",
      "Epoch 164/500 - Validation Loss: 0.2761, Validation Accuracy: 0.8971\n",
      "Epoch [165/500], Train Loss: 0.1012, Train Accuracy: 95.85%\n",
      "Epoch 165/500 - Validation Loss: 0.4225, Validation Accuracy: 0.8603\n",
      "Epoch [166/500], Train Loss: 0.0998, Train Accuracy: 96.08%\n",
      "Epoch 166/500 - Validation Loss: 0.2778, Validation Accuracy: 0.9118\n",
      "Epoch [167/500], Train Loss: 0.1108, Train Accuracy: 95.07%\n",
      "Epoch 167/500 - Validation Loss: 0.2420, Validation Accuracy: 0.9154\n",
      "Best model saved at epoch 167 with val acc: 0.9154\n",
      "Epoch [168/500], Train Loss: 0.0995, Train Accuracy: 95.16%\n",
      "Epoch 168/500 - Validation Loss: 0.3072, Validation Accuracy: 0.8934\n",
      "Epoch [169/500], Train Loss: 0.1003, Train Accuracy: 96.18%\n",
      "Epoch 169/500 - Validation Loss: 0.3107, Validation Accuracy: 0.8952\n",
      "Epoch [170/500], Train Loss: 0.1008, Train Accuracy: 96.04%\n",
      "Epoch 170/500 - Validation Loss: 0.2954, Validation Accuracy: 0.8897\n",
      "Epoch [171/500], Train Loss: 0.0856, Train Accuracy: 96.59%\n",
      "Epoch 171/500 - Validation Loss: 0.3507, Validation Accuracy: 0.8860\n",
      "Epoch [172/500], Train Loss: 0.1118, Train Accuracy: 95.02%\n",
      "Epoch 172/500 - Validation Loss: 0.4260, Validation Accuracy: 0.8474\n",
      "Epoch [173/500], Train Loss: 0.1061, Train Accuracy: 95.90%\n",
      "Epoch 173/500 - Validation Loss: 0.3176, Validation Accuracy: 0.8842\n",
      "Epoch [174/500], Train Loss: 0.0934, Train Accuracy: 96.27%\n",
      "Epoch 174/500 - Validation Loss: 0.3060, Validation Accuracy: 0.8971\n",
      "Epoch [175/500], Train Loss: 0.1029, Train Accuracy: 95.21%\n",
      "Epoch 175/500 - Validation Loss: 0.3414, Validation Accuracy: 0.8952\n",
      "Epoch [176/500], Train Loss: 0.0918, Train Accuracy: 96.18%\n",
      "Epoch 176/500 - Validation Loss: 0.2692, Validation Accuracy: 0.8952\n",
      "Epoch [177/500], Train Loss: 0.1034, Train Accuracy: 95.90%\n",
      "Epoch 177/500 - Validation Loss: 0.2951, Validation Accuracy: 0.9007\n",
      "Epoch [178/500], Train Loss: 0.0808, Train Accuracy: 96.41%\n",
      "Epoch 178/500 - Validation Loss: 0.3771, Validation Accuracy: 0.8879\n",
      "Epoch [179/500], Train Loss: 0.0922, Train Accuracy: 95.71%\n",
      "Epoch 179/500 - Validation Loss: 0.2915, Validation Accuracy: 0.9081\n",
      "Epoch [180/500], Train Loss: 0.1042, Train Accuracy: 95.39%\n",
      "Epoch 180/500 - Validation Loss: 0.2745, Validation Accuracy: 0.9026\n",
      "Epoch [181/500], Train Loss: 0.1155, Train Accuracy: 95.07%\n",
      "Epoch 181/500 - Validation Loss: 0.2869, Validation Accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Initialize TensorBoard\n",
    "writer = SummaryWriter(log_dir='./runs/convLSTM')\n",
    "# Variables to keep track of the best validation accuracy\n",
    "best_val_acc = 0.0\n",
    "best_model_path = '7-segment_datasets/highRel_weights_back_all/best_model.pth'\n",
    "last_model_path = '7-segment_datasets/highRel_weights_back_all/last_model.pth'\n",
    "num_epochs = 500\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for keypoints, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(keypoints)  # outputs shape should be [batch_size, 1]\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('outputs', outputs)\n",
    "        # Calculate accuracy\n",
    "        predicted_labels = outputs.round()  # Round the outputs to get the final predictions for binary classification\n",
    "        # print('predicted_labels',predicted_labels, 'True labels:', labels)\n",
    "        correct_predictions += (predicted_labels == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "\n",
    "        # Calculate accuracy and loss for the epoch\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # Perform validation\n",
    "    val_loss, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # Log loss and accuracy to TensorBoard\n",
    "    writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    \n",
    "    # Print validation metrics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} - Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the last model weights\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "    \n",
    "    # Save the best model weights based on validation accuracy\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f'Best model saved at epoch {epoch+1} with val acc: {val_accuracy:.4f}')\n",
    "\n",
    "print(f'Best validation accuracy: {best_val_acc:.4f}')\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443479/2383594405.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('7-segment_datasets/highRel_weights/best_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('7-segment_datasets/highRel_weights/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9425287356321839\n",
      "Recall:  0.9111111111111111\n",
      "f1-score:  0.9265536723163842\n",
      "Average Loss: 0.36838574798661033, Accuracy: 0.9171974522292994\n",
      "Confusion Matrix:\n",
      " [[186  15]\n",
      " [ 24 246]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function for evaluating the model on the validation set\n",
    "def evaluate_with_confusion_matrix(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients for validation\n",
    "        for keypoints, labels in val_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(keypoints)  # outputs shape should be [batch_size, 1]\n",
    "            # outputs = outputs.squeeze()\n",
    "            # labels = labels.squeeze().long()\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predicted_labels = outputs.round()  # This should be along dimension 1 for batch data\n",
    "            all_labels.append(labels.tolist()[0][0])\n",
    "            all_predictions.append(predicted_labels.tolist()[0][0])\n",
    "            \n",
    "        # Calculate average loss\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    precision = precision_score(all_labels, all_predictions, average='binary')\n",
    "    recall = recall_score(all_labels, all_predictions, average='binary')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('f1-score: ', f1)\n",
    "    return avg_loss, accuracy, cm\n",
    "\n",
    "# Use the function\n",
    "avg_loss, accuracy, cm = evaluate_with_confusion_matrix(model, val_loader, criterion)\n",
    "print(f\"Average Loss: {avg_loss}, Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with z point:\n",
    "Precision:  0.9277566539923955\n",
    "Recall:  0.9037037037037037\n",
    "f1-score:  0.9155722326454033\n",
    "Average Loss: 0.43734880255015096, Accuracy: 0.9044585987261147\n",
    "Confusion Matrix:\n",
    " [[182  19]\n",
    " [ 26 244]]\n",
    "\n",
    "without ball points:\n",
    "Precision:  0.9427480916030534\n",
    "Recall:  0.9148148148148149\n",
    "f1-score:  0.9285714285714286\n",
    "Average Loss: 0.427513430891732, Accuracy: 0.9193205944798302\n",
    "Confusion Matrix:\n",
    " [[186  15]\n",
    " [ 23 247]]\n",
    "\n",
    "with Ball points and no z point:\n",
    "Precision:  0.9425287356321839\n",
    "Recall:  0.9111111111111111\n",
    "f1-score:  0.9265536723163842\n",
    "Average Loss: 0.36838574798661033, Accuracy: 0.9171974522292994\n",
    "Confusion Matrix:\n",
    " [[186  15]\n",
    " [ 24 246]]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basketball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
